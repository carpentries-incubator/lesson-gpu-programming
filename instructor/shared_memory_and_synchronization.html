<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>GPU Programming: Shared Memory and Synchronization</title><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png"><link rel="manifest" href="../site.webmanifest"><link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" content="#ffffff"></head><body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team." style="background-color: #001483; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link" style="color: #FFF7F1">
            <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
            Beta
          </a>
          <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../shared_memory_and_synchronization.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg"></div>
    <div class="lesson-title-md">
      GPU Programming
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            GPU Programming
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="../aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  GPU Programming
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 78%" class="percentage">
    78%
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: 78%" aria-valuenow="78" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../shared_memory_and_synchronization.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->
      
            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="introduction.html">1. Introduction</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="cupy.html">2. Using your GPU with CuPy</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="numba.html">3. Accelerate your Python code with Numba</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="gpu_introduction.html">4. A Better Look at the GPU</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="first_program.html">5. Your First GPU Kernel</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="global_local_memory.html">6. Registers, Global, and Local Memory</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        7. Shared Memory and Synchronization
        </span>
      
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="constant_memory.html">8. Constant Memory</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="streams.html">9. Concurrent access to the GPU</a>
    </div><!--/div.accordion-header-->
        
  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width resources"><a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">
            
            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/global_local_memory.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/constant_memory.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/global_local_memory.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Registers, Global,
        </a>
        <a class="chapter-link float-end" href="../instructor/constant_memory.html" rel="next">
          Next: Constant Memory... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Shared Memory and Synchronization</h1>
        <p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/shared_memory_and_synchronization.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
        
        
        
        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 55 minutes</p>
        
        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>“Is there a way to share data between threads of a same block?”</li>
<li>“Can threads inside a block wait for other threads?”</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>“Learn how to share data between threads”</li>
<li>“Learn how to synchronize threads”</li>
</ul></div>
</div>
</div>
</div>
</div>
<p>So far we looked at how to use CUDA to accelerate the computation,
but a common pattern in all the examples we encountered so far is that
threads worked in isolation. While having different threads perform the
same operation on different data is a good pattern for working with
GPUs, there are cases in which threads need to communicate. This
communication may be necessary because of the way the algorithm we are
trying to implement works, or it may derive from a performance goal we
are trying to achieve.</p>
<div class="section level1">
<h1 id="shared-memory">Shared Memory<a class="anchor" aria-label="anchor" href="#shared-memory"></a></h1>
<p>Shared memory is a CUDA memory space that is shared by all threads in
a thread block. In this case <em>shared</em> means that all threads in a
thread block can write and read to block-allocated shared memory, and
all changes to this memory will be eventually available to all threads
in the block.</p>
<p>To allocate an array in shared memory we need to preface the
definition with the identifier <code>__shared__</code>.</p>
<div id="challenge-use-of-shared-memory" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-use-of-shared-memory" class="callout-inner">
<h3 class="callout-title">Challenge: use of shared memory<a class="anchor" aria-label="anchor" href="#challenge-use-of-shared-memory"></a>
</h3>
<div class="callout-content">
<p>Modify the following code to allocate the <code>temp</code> array in
shared memory.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>  <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>To use shared memory for the <code>temp</code> array add the
identifier <code>__shared__</code> to its definition, like in the
following code.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>  <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>  __shared__ <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="op">&gt;</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>While syntactically correct, the previous example is functionally
wrong. The reason is that the <code>temp</code> array is not anymore
private to the thread allocating it, but it is now shared by the whole
thread block.</p>
<div id="challenge-what-is-the-result-of-the-previous-code-block" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-what-is-the-result-of-the-previous-code-block" class="callout-inner">
<h3 class="callout-title">Challenge: what is the result of the previous
code block?<a class="anchor" aria-label="anchor" href="#challenge-what-is-the-result-of-the-previous-code-block"></a>
</h3>
<div class="callout-content">
<p>The previous code example is functionally wrong. Can you guess what
the result of its execution will be?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>The result is non deterministic, and definitely not the same as the
previous versions of <code>vector_add</code>. Threads will overwrite
each other temporary values,and there will be no guarantee on which
value is visible by each thread.</p>
</div>
</div>
</div>
</div>
<p>To fix the previous kernel we should allocate enough shared memory
for each thread to store three values, so that each thread has its own
section of the shared memory array to work with.</p>
<p>To allocate enough memory we need to replace the constant 3 in
<code>__shared__ float temp[3]</code> with something else. If we know
that each thread block has 1024 threads, we can write something like the
following:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>__shared__ <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span> <span class="op">*</span> <span class="dv">1024</span><span class="op">];</span></span></code></pre>
</div>
<p>But we know by experience that having constants in the code is not a
scalable and maintainable solution. The problem is that we need to have
a constant value if we want to declare a shared memory array, because
the compiler needs to know how much memory to allocate.</p>
<p>A solution to this problem is to not specify the size of the array,
and allocate the memory somewhere else.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">extern</span> __shared__ <span class="dt">float</span> temp<span class="op">[];</span></span></code></pre>
</div>
<p>And then use CuPy to instruct CUDA about how much shared memory, in
bytes, each thread block needs. This can be done by adding the named
parameter <code>shared_mem</code> to the kernel call.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>vector_add_gpu((<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size), shared_mem<span class="op">=</span>((size <span class="op">//</span> <span class="dv">2</span>) <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> cupy.dtype(cupy.float32).itemsize))</span></code></pre>
</div>
<p>As you may have noticed, we had to retrieve the size in bytes of the
data type <code>cupy.float32</code>, and this is done with
<code>cupy.dtype(cupy.float32).itemsize</code>.</p>
<p>After these changes, the body of the kernel needs to be modified to
use the right indices:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>  <span class="dt">int</span> offset <span class="op">=</span> threadIdx<span class="op">.</span>x <span class="op">*</span> <span class="dv">3</span><span class="op">;</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>  <span class="kw">extern</span> __shared__ <span class="dt">float</span> temp<span class="op">[];</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>      temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>      temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>      temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">1</span><span class="op">];</span></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">2</span><span class="op">];</span></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>And for completeness, we present the full Python code.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># vector size</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># GPU memory allocation</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>gpu_args <span class="op">=</span> (a_gpu, b_gpu, c_gpu, size)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="co"># CPU memory allocation</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>a_cpu <span class="op">=</span> cupy.asnumpy(a_gpu)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>b_cpu <span class="op">=</span> cupy.asnumpy(b_gpu)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>c_cpu <span class="op">=</span> np.zeros(size, dtype<span class="op">=</span>np.float32)</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>vector_add_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a><span class="vs">  int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a><span class="vs">  int offset = threadIdx.x * 3;</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="vs">  extern __shared__ float temp[];</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a><span class="vs">  if ( item &lt; size )</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a><span class="vs">  {</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a><span class="vs">      temp[offset + 0] = A[item];</span></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a><span class="vs">      temp[offset + 1] = B[item];</span></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a><span class="vs">      temp[offset + 2] = temp[offset + 0] + temp[offset + 1];</span></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a><span class="vs">      C[item] = temp[offset + 2];</span></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a><span class="vs">  }</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a><span class="co"># compile and execute code</span></span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(vector_add_cuda_code, <span class="st">"vector_add"</span>)</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>vector_add_gpu(grid_size, block_size, gpu_args, shared_mem<span class="op">=</span>(threads_per_block <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> cupy.dtype(cupy.float32).itemsize))</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a><span class="co"># execute Python code and compare results</span></span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>vector_add(a_cpu, b_cpu, c_cpu, size)</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>np.allclose(c_cpu, c_gpu)</span></code></pre>
</div>
<p>The code is now correct, although it is still not very useful. We are
definitely using shared memory, and we are using it the correct way, but
there is no performance gain we achieved by doing so. Actually, we are
making our code slower, not faster, because shared memory is slower than
registers.</p>
<p>Let us, therefore, work on an example where using shared memory is
actually useful. We start again with some Python code.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> histogram(input_array, output_array):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> input_array:</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        output_array[item] <span class="op">=</span> output_array[item] <span class="op">+</span> <span class="dv">1</span></span></code></pre>
</div>
<p>The <code>histogram</code> function, as the name suggests, computes
the histogram of an array of integers, i.e. counts how many instances of
each integer are in <code>input_array</code>, and writes the count in
<code>output_array</code>. We can now generate some data and run the
code.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>input_array <span class="op">=</span> np.random.randint(<span class="dv">256</span>, size<span class="op">=</span><span class="dv">2048</span>, dtype<span class="op">=</span>np.int32)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>output_array <span class="op">=</span> np.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>np.int32)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>histogram(input_array, output_array)</span></code></pre>
</div>
<p>Everything as expected. We can now write the equivalent code in
CUDA.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">=</span> output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<div id="challenge-error-in-the-histogram" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-error-in-the-histogram" class="callout-inner">
<h3 class="callout-title">Challenge: error in the histogram<a class="anchor" aria-label="anchor" href="#challenge-error-in-the-histogram"></a>
</h3>
<div class="callout-content">
<p>If you look at the CUDA <code>histogram</code> code, there is a
logical error that prevents it to produce the correct results. Can you
find it?</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">=</span> output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>The GPU is a highly parallel device, executing multiple threads at
the same time. In the previous code different threads could be updating
the same output item at the same time, producing wrong results.</p>
</div>
</div>
</div>
</div>
<p>To solve this problem, we need to use a function from the CUDA
library named <code>atomicAdd</code>. This function ensures that the
increment of <code>output_array</code> happens in an atomic way, so that
there are no conflicts in case multiple threads want to update the same
item at the same time.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>And the full Python code snippet.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="kw">def</span> histogram(input_array, output_array):</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> input_array:</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>        output_array[item] <span class="op">=</span> output_array[item] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="co"># input size</span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">25</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a><span class="co"># allocate memory on CPU and GPU</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>input_gpu <span class="op">=</span> cupy.random.randint(<span class="dv">256</span>, size<span class="op">=</span>size, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>input_cpu <span class="op">=</span> cupy.asnumpy(input_gpu)</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>output_gpu <span class="op">=</span> cupy.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>output_cpu <span class="op">=</span> cupy.asnumpy(output_gpu)</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>histogram_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a><span class="vs">__global__ void histogram(const int * input, int * output)</span></span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(output[input[item]]), 1);</span></span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb13-28"><a href="#cb13-28" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb13-29"><a href="#cb13-29" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" tabindex="-1"></a><span class="co"># compile and setup CUDA code</span></span>
<span id="cb13-31"><a href="#cb13-31" tabindex="-1"></a>histogram_gpu <span class="op">=</span> cupy.RawKernel(histogram_cuda_code, <span class="st">"histogram"</span>)</span>
<span id="cb13-32"><a href="#cb13-32" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb13-33"><a href="#cb13-33" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-34"><a href="#cb13-34" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-35"><a href="#cb13-35" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" tabindex="-1"></a><span class="co"># check correctness</span></span>
<span id="cb13-37"><a href="#cb13-37" tabindex="-1"></a>histogram(input_cpu, output_cpu)</span>
<span id="cb13-38"><a href="#cb13-38" tabindex="-1"></a>histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))</span>
<span id="cb13-39"><a href="#cb13-39" tabindex="-1"></a><span class="cf">if</span> np.allclose(output_cpu, output_gpu):</span>
<span id="cb13-40"><a href="#cb13-40" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb13-41"><a href="#cb13-41" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb13-42"><a href="#cb13-42" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span>
<span id="cb13-43"><a href="#cb13-43" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" tabindex="-1"></a><span class="co"># measure performance</span></span>
<span id="cb13-45"><a href="#cb13-45" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> histogram(input_cpu, output_cpu)</span>
<span id="cb13-46"><a href="#cb13-46" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(histogram_gpu, (grid_size, block_size, (input_gpu, output_gpu)), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-47"><a href="#cb13-47" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb13-48"><a href="#cb13-48" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>The CUDA code is now correct, and computes the same result as the
Python code; it is also faster than the Python code, as you can see from
measuring the execution time. However, we are accumulating the results
directly in global memory, and the more conflicts we have in global
memory, the lower the performance of our <code>histogram</code> will be.
Moreover, the access pattern to the output array is very irregular,
being dependent on the content of the input array. GPUs are designed for
very regular computations, and so if we can make the histogram more
regular we can hope in a further improvement in performance.</p>
<p>As you may expect, we can improve the memory access pattern by using
shared memory.</p>
<div id="challenge-use-shared-memory-to-speed-up-the-histogram" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-use-shared-memory-to-speed-up-the-histogram" class="callout-inner">
<h3 class="callout-title">Challenge: use shared memory to speed up the
histogram<a class="anchor" aria-label="anchor" href="#challenge-use-shared-memory-to-speed-up-the-histogram"></a>
</h3>
<div class="callout-content">
<p>Implement a new version of the CUDA <code>histogram</code> function
that uses shared memory to reduce conflicts in global memory. Modify the
following code and follow the suggestions in the comments.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    <span class="co">// Declare temporary histogram in shared memory</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>    <span class="dt">int</span> temp_histogram<span class="op">[</span><span class="dv">256</span><span class="op">];</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>    <span class="co">// Update the temporary histogram in shared memory</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    atomicAdd<span class="op">();</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>    <span class="co">// Update the global histogram in global memory, using the temporary histogram</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>    atomicAdd<span class="op">();</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>Hint: for this exercise, you can safely assume that the size of
<code>output</code> is the same as the number of threads in a block.</p>
<p>Hint: <code>atomicAdd</code> can be used on both global and shared
memory.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>The following code shows one of the possible solutions.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    __shared__ <span class="dt">int</span> temp_histogram<span class="op">[</span><span class="dv">256</span><span class="op">];</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>temp_histogram<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]),</span> temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]);</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The idea behind this solution is to reduce the expensive conflicts in
global memory by having a temporary histogram in shared memory. After a
block has finished processing its fraction of the input array, and the
local histogram is populated, threads collaborate to update the global
histogram. Not only this solution potentially reduces the conflicts in
global memory, it also produces a better access pattern because threads
read adjacent items of the <code>input</code> array, and write to
adjacent elements of the <code>output</code> array during the second
call to <code>atomicAdd</code>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="thread-synchronization">Thread Synchronization<a class="anchor" aria-label="anchor" href="#thread-synchronization"></a></h1>
<p>There is still one potentially big issue in the
<code>histogram</code> code we just wrote, and the issue is that shared
memory is not coherent without explicit synchronization. The problem
lies in the following two lines of code:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>atomicAdd<span class="op">(&amp;(</span>temp_histogram<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]),</span> temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]);</span></span></code></pre>
</div>
<p>In the first line each thread updates one arbitrary position in
shared memory, depending on the value of the input, while in the second
line each thread reads the element in shared memory corresponding to its
thread ID. However, the changes to shared memory are not automatically
available to all other threads, and therefore the final result may not
be correct.</p>
<p>To solve this issue, we need to explicitly synchronize all threads in
a block, so that memory operations are also finalized and visible to
all. To synchronize threads in a block, we use the
<code>__syncthreads()</code> CUDA function. Moreover, shared memory is
not initialized, and the programmer needs to take care of that too. So
we need to first initialize <code>temp_histogram</code>, wait that all
threads are done doing this, perform the computation in shared memory,
wait again that all threads are done, and only then update the global
array.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>    __shared__ <span class="dt">int</span> temp_histogram<span class="op">[</span><span class="dv">256</span><span class="op">];</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a> </span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>    <span class="co">// Initialize shared memory and synchronize</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>    temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    __syncthreads<span class="op">();</span></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>    <span class="co">// Compute shared memory histogram and synchronize</span></span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>temp_histogram<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>    __syncthreads<span class="op">();</span></span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>    <span class="co">// Update global histogram</span></span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]),</span> temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]);</span></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>And the full Python code snippet.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="kw">def</span> histogram(input_array, output_array):</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> input_array:</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>        output_array[item] <span class="op">=</span> output_array[item] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="co"># input size</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">25</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a><span class="co"># allocate memory on CPU and GPU</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>input_gpu <span class="op">=</span> cupy.random.randint(<span class="dv">256</span>, size<span class="op">=</span>size, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a>input_cpu <span class="op">=</span> cupy.asnumpy(input_gpu)</span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>output_gpu <span class="op">=</span> cupy.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>output_cpu <span class="op">=</span> cupy.asnumpy(output_gpu)</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>histogram_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a><span class="vs">__global__ void histogram(const int * input, int * output)</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a><span class="vs">    __shared__ int temp_histogram[256];</span></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a><span class="vs"> </span></span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a><span class="vs">    // Initialize shared memory and synchronize</span></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a><span class="vs">    temp_histogram[threadIdx.x] = 0;</span></span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a><span class="vs">    __syncthreads();</span></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a><span class="vs">    // Compute shared memory histogram and synchronize</span></span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(temp_histogram[input[item]]), 1);</span></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a><span class="vs">    __syncthreads();</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a><span class="vs">    // Update global histogram</span></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(output[threadIdx.x]), temp_histogram[threadIdx.x]);</span></span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb18-39"><a href="#cb18-39" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" tabindex="-1"></a><span class="co"># compile and setup CUDA code</span></span>
<span id="cb18-41"><a href="#cb18-41" tabindex="-1"></a>histogram_gpu <span class="op">=</span> cupy.RawKernel(histogram_cuda_code, <span class="st">"histogram"</span>)</span>
<span id="cb18-42"><a href="#cb18-42" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb18-43"><a href="#cb18-43" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-44"><a href="#cb18-44" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-45"><a href="#cb18-45" tabindex="-1"></a></span>
<span id="cb18-46"><a href="#cb18-46" tabindex="-1"></a><span class="co"># check correctness</span></span>
<span id="cb18-47"><a href="#cb18-47" tabindex="-1"></a>histogram(input_cpu, output_cpu)</span>
<span id="cb18-48"><a href="#cb18-48" tabindex="-1"></a>histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))</span>
<span id="cb18-49"><a href="#cb18-49" tabindex="-1"></a><span class="cf">if</span> np.allclose(output_cpu, output_gpu):</span>
<span id="cb18-50"><a href="#cb18-50" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb18-51"><a href="#cb18-51" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-52"><a href="#cb18-52" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span>
<span id="cb18-53"><a href="#cb18-53" tabindex="-1"></a></span>
<span id="cb18-54"><a href="#cb18-54" tabindex="-1"></a><span class="co"># measure performance</span></span>
<span id="cb18-55"><a href="#cb18-55" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> histogram(input_cpu, output_cpu)</span>
<span id="cb18-56"><a href="#cb18-56" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(histogram_gpu, (grid_size, block_size, (input_gpu, output_gpu)), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-57"><a href="#cb18-57" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb18-58"><a href="#cb18-58" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>While both versions of the GPU histogram are correct, the one using
shared memory is faster; but how fast? On a NVIDIA Tesla T4 accessed via
Google Colab, the shared memory version is ten times faster than the
version doing atomic operations on global memory.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul><li>“Shared memory is faster than global memory and local memory”</li>
<li>“Shared memory can be used as a user-controlled cache to speedup
code”</li>
<li>“Size of shared memory arrays must be known at compile time if
allocated inside a thread”</li>
<li>“It is possible to declare <code>extern</code> shared memory arrays
and pass the size during kernel invocation”</li>
<li>“Use <code>__shared__</code> to allocate memory in the shared memory
space”</li>
<li>“Use <code>__syncthreads()</code> to wait for shared memory
operations to be visible to all threads in a block”</li>
</ul></div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>



      </div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/global_local_memory.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/constant_memory.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/global_local_memory.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Registers, Global,
        </a>
        <a class="chapter-link float-end" href="../instructor/constant_memory.html" rel="next">
          Next: Constant Memory... 
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/shared_memory_and_synchronization.Rmd" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/lesson-gpu-programming/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:instructors@esciencecenter.nl">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/lesson-gpu-programming/instructor/shared_memory_and_synchronization.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "high-performance computing, HPC, graphics processing units, GPU",
  "name": "Shared Memory and Synchronization",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/lesson-gpu-programming/instructor/shared_memory_and_synchronization.html",
  "identifier": "https://carpentries-incubator.github.io/lesson-gpu-programming/instructor/shared_memory_and_synchronization.html",
  "dateCreated": "2020-09-25",
  "dateModified": "2024-03-12",
  "datePublished": "2024-05-07"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code --></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

