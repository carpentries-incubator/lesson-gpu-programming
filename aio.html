<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>GPU Programming: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="assets/styles.css">
<script src="assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav incubator"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="Carpentries Incubator" src="assets/images/incubator-logo.svg"><abbr class="badge badge-light" title="This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team." style="background-color: #001483; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#polishing-beta-stage" class="external-link alert-link" style="color: #FFF7F1">
            <i aria-hidden="true" class="icon" data-feather="alert-circle" style="border-radius: 5px"></i>
            Beta
          </a>
          <span class="visually-hidden">This lesson is in the beta phase, which means that it is ready for teaching by instructors outside of the original author team.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Learner View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='instructor/aio.html';">Instructor View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      GPU Programming
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            GPU Programming
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="reference.html#glossary">Glossary</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="profiles.html">Learner Profiles</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a class="btn btn-primary" href="aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  GPU Programming
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Learner View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="instructor/aio.html">Instructor View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Setup</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="introduction.html">1. Introduction</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="cupy.html">2. Using your GPU with CuPy</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="numba.html">3. Accelerate your Python code with Numba</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="gpu_introduction.html">4. A Better Look at the GPU</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="first_program.html">5. Your First GPU Kernel</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="global_local_memory.html">6. Registers, Global, and Local Memory</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="shared_memory_and_synchronization.html">7. Shared Memory and Synchronization</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="constant_memory.html">8. Constant Memory</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="streams.html">9. Concurrent access to the GPU</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="reference.html#glossary">Glossary</a>
                      </li>
                      <li>
                        <a href="profiles.html">Learner Profiles</a>
                      </li>
                      <li><a href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-introduction"><p>Content from <a href="introduction.html">Introduction</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/introduction.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“What is a Graphics Processing Unit?”</li>
<li>“Can a GPU be used for anything else than graphics?”</li>
<li>“Are GPUs faster than CPUs?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Understand the differences between CPU and GPU”</li>
<li>“See the possible performance benefits of GPU acceleration”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="graphics-processing-unit">Graphics Processing Unit<a class="anchor" aria-label="anchor" href="#graphics-processing-unit"></a>
</h1>
<p>The Graphics Processing Unit (<strong>GPU</strong>) is one of the
components of a computer’s video card, together with specialized memory
and different Input/Output (I/O) units. In the context of the video
card, the GPU fulfills a role similar to the one that the Central
Processing Unit (<strong>CPU</strong>) has in a general purpose
computing system: it processes input data to generate some kind of
output. In the traditional context of video cards, GPUs process data in
order to render images on an output device, such as a screen. However,
modern GPUs are general purpose computing devices that can be used to
perform any kind of computation, and this is what we are going to do in
this lesson.</p>
</div>
<div class="section level1">
<h1 id="parallel-by-design">Parallel by Design<a class="anchor" aria-label="anchor" href="#parallel-by-design"></a>
</h1>
<p>But what is the reason to use GPUs to perform general purpose
computation, when computers already have fast CPUs that are able to
perform any kind of computation? One way to answer this question is to
go back to the roots of what a GPU is designed to do: render images.</p>
<p>An image can be seen as a matrix of points called
<strong>pixels</strong> (a portmanteau of the words <em>picture</em> and
<em>element</em>), with each pixel representing the color the image
should have in that particular point, and the traditional task performed
by video cards is to produce the images a user will see on the screen. A
single 4K UHD image contains more than 8 million pixels. For a GPU to
render a continuous stream of 25 4K frames (images) per second, enough
so that users not experience delay in a videogame, movie, or any other
video output, it must process over 200 million pixels per second. So
GPUs are designed to render multiple pixels at the same time, and they
are designed to do it efficiently. This design principle results in the
GPU being, from a hardware point of view, very different from a CPU.</p>
<p>The CPU is a very general purpose device, good at different tasks,
being them parallel or sequential in nature; it is also designed for
interaction with the user, so it has to be responsive and guarantee
minimal latency. In practice, we want our CPU to be available whenever
we sent it a new task. The result is a device where most of the silicon
is used for memory caches and control-flow logic, not just compute
units.</p>
<p>By contrast, most of the silicon on a GPU is actually used for
compute units. The GPU does not need an overly complicated cache
hierarchy, nor does it need complex control logic, because the overall
goal is not to minimize the latency of any given thread, but to maximize
the throughput of the whole computation. With many compute units
available, the GPU can run massively parallel programs, programs in
which thousands of threads are executed at the same time, while
thousands more are ready for execution to hide the cost of memory
operations.</p>
<p>A high-level introduction on the differences between CPU and GPU can
also be found in the following YouTube video.</p>
<p><a href="https://www.youtube.com/watch?v=bZdxcHEM-uc" class="external-link"><img src="https://img.youtube.com/vi/bZdxcHEM-uc/0.jpg" alt="Screenshot of the YoutTube video showing a GPU" class="figure"></a></p>
</div>
<div class="section level1">
<h1 id="speed-benefits">Speed Benefits<a class="anchor" aria-label="anchor" href="#speed-benefits"></a>
</h1>
<p>So, GPUs are massively parallel devices that can execute thousands of
threads at the same time. But what does it mean in practice for the
user? Why anyone would need to use a GPU to compute something that can
be easily computed on a CPU? We can try to answer this question with an
example.</p>
<p>Suppose we want to sort a large array in Python. Using NumPy, we
first need to create an array of random single precision floating point
numbers.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">4096</span> <span class="op">*</span> <span class="dv">4096</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> np.random.random(size).astype(np.float32)</span></code></pre>
</div>
<p>We then time the execution of the NumPy <code>sort()</code> function,
to see how long sorting this array takes on the CPU.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> output <span class="op">=</span> np.sort(<span class="bu">input</span>)</span></code></pre>
</div>
<p>While the timing of this operation will differ depending on the
system on which you run the code, these are the results for one
experiment running on a Jupyter notebook on Google Colab.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>1.83 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)</code></pre>
</div>
<p>We now perform the same sorting operation, but this time we will be
using CuPy to execute the <code>sort()</code> function on the GPU. CuPy
is an open-source library, compatible with NumPy, for GPU computing in
Python.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">import</span> cupy <span class="im">as</span> cp</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>input_gpu <span class="op">=</span> cp.asarray(<span class="bu">input</span>)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(cp.sort, (input_gpu,), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>We also report the output, obtained on the same notebook on Google
Colab; as always note that your result will vary based on the
environment and GPU you are using.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>0.008949 s</code></pre>
</div>
<p>Notice that the first thing we need to do, is to copy the input data
to the GPU. The distinction between data on the GPU and that on the
<em>host</em> is a very important one that we will get back to
later.</p>
<div id="host-vs.-device" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="host-vs.-device" class="callout-inner">
<h3 class="callout-title">Host vs. Device<a class="anchor" aria-label="anchor" href="#host-vs.-device"></a>
</h3>
<div class="callout-content">
<p>From now on we may also call the GPU the <em>device</em>, while we
refer to other computations as taking place on the <em>host</em>. We’ll
also talk about <em>host memory</em> and <em>device memory</em>, but
much more on memory in later episodes!</p>
</div>
</div>
</div>
<p>Sorting an array using CuPy, and therefore using the GPU, is clearly
much faster than using NumPy, but can we quantify how much faster?
Having recorded the average execution time of both operations, we can
then compute the speedup of using CuPy over NumPy. The speedup is
defined as the ratio between the sequential (NumPy in our case) and
parallel (CuPy in our case) execution times; beware that both execution
times need to be in the same unit, this is why we had to convert the GPU
execution time from milliseconds to seconds.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>speedup <span class="op">=</span> <span class="fl">1.83</span> <span class="op">/</span> <span class="fl">0.008949</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(speedup)</span></code></pre>
</div>
<p>With the result of the previous operation being the following.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">204.49212202480723</span></span></code></pre>
</div>
<p>We can therefore say that just by using the GPU with CuPy to sort an
array of size <code>4096 * 4096</code> we achieved a performance
improvement of 204 times.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“CPUs and GPUs are both useful and each has its own place in our
toolbox”</li>
<li>“In the context of GPU programming, we often refer to the GPU as the
<em>device</em> and the CPU as the <em>host</em>”</li>
<li>“Using GPUs to accelerate computation can provide large performance
gains”</li>
<li>“Using the GPU with Python is not particularly difficult”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-cupy"><p>Content from <a href="cupy.html">Using your GPU with CuPy</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/cupy.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“How can I increase the performance of code that uses NumPy?”</li>
<li>“How can I copy NumPy arrays to the GPU?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Be able to indicate if an array, represented by a variable in an
iPython shell, is stored in host or device memory.”</li>
<li>“Be able to copy the contents of this array from host to device
memory and vice versa.”</li>
<li>“Be able to select the appropriate function to either convolve an
image using either CPU or GPU compute power.”</li>
<li>“Be able to quickly estimate the speed benefits for a simple
calculation by moving it from the CPU to the GPU.”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="introduction-to-cupy">Introduction to CuPy<a class="anchor" aria-label="anchor" href="#introduction-to-cupy"></a>
</h1>
<p><a href="https://cupy.dev" class="external-link">CuPy</a> is a GPU array library that
implements a subset of the NumPy and SciPy interfaces. This makes it a
very convenient tool to use the compute power of GPUs for people that
have some experience with NumPy, without the need to write code in a GPU
programming language such as CUDA, OpenCL, or HIP.</p>
</div>
<div class="section level1">
<h1 id="convolution-in-python">Convolution in Python<a class="anchor" aria-label="anchor" href="#convolution-in-python"></a>
</h1>
<p>We start by generating an artificial “image” on the host using Python
and NumPy; the host is the CPU on the laptop, desktop, or cluster node
you are using right now, and from now on we may use <em>host</em> to
refer to the CPU and <em>device</em> to refer to the GPU. The image will
be all zeros, except for isolated pixels with value one, on a regular
grid. The plan is to convolve it with a Gaussian and inspect the result.
We will also record the time it takes to execute this convolution on the
host.</p>
<p>We can interactively write and executed the code in an iPython shell
or a Jupyter notebook.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># Construct an image with repeated delta functions</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>deltas <span class="op">=</span> np.zeros((<span class="dv">2048</span>, <span class="dv">2048</span>))</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>deltas[<span class="dv">8</span>::<span class="dv">16</span>,<span class="dv">8</span>::<span class="dv">16</span>] <span class="op">=</span> <span class="dv">1</span></span></code></pre>
</div>
<p>To get a feeling of how the whole image looks like, we can display
the top-left corner of it.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> pylab <span class="im">as</span> pyl</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># Necessary command to render a matplotlib image in a Jupyter notebook.</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co"># Display the image</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co"># You can zoom in using the menu in the window that will appear</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>pyl.imshow(deltas[<span class="dv">0</span>:<span class="dv">32</span>, <span class="dv">0</span>:<span class="dv">32</span>])</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>pyl.show()</span></code></pre>
</div>
<p>After executing the code, you should see the following image.</p>
<figure><img src="./fig/deltas.png" alt="Deltas array" class="figure mx-auto d-block"><div class="figcaption">Deltas array</div>
</figure><div class="section level3">
<h3 id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h3>
<p>The computation we want to perform on this image is a convolution,
once on the host and once on the device so we can compare the results
and execution times. In computer vision applications, convolutions are
often used to filter images and if you want to know more about them, we
encourage you to check out <a href="https://github.com/vdumoulin/conv_arithmetic" class="external-link">this github
repository</a> by Vincent Dumoulin and Francesco Visin with some great
animations. We have already seen that we can think of an image as a
matrix of color values, when we convolve that image with a particular
filter, we generate a new matrix with different color values. An example
of convolution can be seen in the figure below (illustration by Michael
Plotke, CC BY-SA 3.0, via Wikimedia Commons).</p>
<figure><img src="./fig/2D_Convolution_Animation.gif" alt="Example of animated convolution" class="figure mx-auto d-block"><div class="figcaption">Example of animated convolution</div>
</figure><p>In our example, we will convolve our image with a 2D Gaussian
function shown below:</p>
<p><span class="math display">\[G(x,y) = \frac{1}{2\pi \sigma^2}
\exp\left(-\frac{x^2 + y^2}{2 \sigma^2}\right),\]</span></p>
<p>where <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are the “coordinates in our matrix,
i.e. our row and columns, and <span class="math display">\[\sigma\]</span> controls the width of the
Gaussian distribution. Convolving images with 2D Gaussian functions will
change the value of each pixel to be a weighted average of the pixels
around it, thereby”smoothing” the image. Convolving images with a
Gaussian function reduces the noise in the image, which is often
required in <a href="https://en.wikipedia.org/wiki/Gaussian_blur#Edge_detection" class="external-link">edge-detection</a>
since most algorithms to do this are sensitive to noise.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>It is often useful to identify the dataflow inherent in a problem.
Say, if we want to square a list of numbers, all the operations are
independent. The dataflow of a one-to-one operation is called a
<em>map</em>.</p>
<figure><img src="./fig/mapping.svg" alt="Data flow of a map operation" class="figure mx-auto d-block"><div class="figcaption">Data flow of a map operation</div>
</figure><p>A convolution is slightly more complicated. Here we have a
many-to-one data flow, which is also known as a stencil.</p>
<figure><img src="./fig/stencil.svg" alt="Data flow of a stencil operation" class="figure mx-auto d-block"><div class="figcaption">Data flow of a stencil operation</div>
</figure><p>GPU’s are exceptionally well suited to compute algorithms that follow
one of these patterns.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="convolution-on-the-cpu-using-scipy">Convolution on the CPU Using SciPy<a class="anchor" aria-label="anchor" href="#convolution-on-the-cpu-using-scipy"></a>
</h1>
<p>Let us first construct the Gaussian, and then display it. Remember
that at this point we are still doing everything with standard Python,
and not using the GPU yet.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>x, y <span class="op">=</span> np.meshgrid(np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">15</span>), np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">15</span>))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>dst <span class="op">=</span> np.sqrt(x<span class="op">*</span>x <span class="op">+</span> y<span class="op">*</span>y)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>muu <span class="op">=</span> <span class="fl">0.000</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>gauss <span class="op">=</span> np.exp(<span class="op">-</span>((dst<span class="op">-</span>muu)<span class="op">**</span><span class="dv">2</span><span class="op">/</span>(<span class="fl">2.0</span> <span class="op">*</span> sigma<span class="op">**</span><span class="dv">2</span>)))</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>pyl.imshow(gauss)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>pyl.show()</span></code></pre>
</div>
<p>This should show you a symmetrical two-dimensional Gaussian.</p>
<figure><img src="./fig/gauss.png" alt="Two-dimensional Gaussian" class="figure mx-auto d-block"><div class="figcaption">Two-dimensional Gaussian</div>
</figure><p>Now we are ready to do the convolution on the host. We do not have to
write this convolution function ourselves, as it is very conveniently
provided by SciPy. Let us also record the time it takes to perform this
convolution and inspect the top left corner of the convolved image.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> scipy.signal <span class="im">import</span> convolve2d <span class="im">as</span> convolve2d_cpu</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>convolved_image_using_CPU <span class="op">=</span> convolve2d_cpu(deltas, gauss)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>pyl.imshow(convolved_image_using_CPU[<span class="dv">0</span>:<span class="dv">32</span>, <span class="dv">0</span>:<span class="dv">32</span>])</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>pyl.show()</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> convolve2d_cpu(deltas, gauss)</span></code></pre>
</div>
<p>Obviously, the time to perform this convolution will depend very much
on the power of your CPU, but I expect you to find that it takes a
couple of seconds.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>2.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)</code></pre>
</div>
<p>When you display the corner of the image, you can see that the “ones”
surrounded by zeros have actually been blurred by a Gaussian, so we end
up with a regular grid of Gaussians.</p>
<figure><img src="./fig/convolved_image.png" alt="Regular grid of Gaussians" class="figure mx-auto d-block"><div class="figcaption">Regular grid of Gaussians</div>
</figure>
</div>
<div class="section level1">
<h1 id="convolution-on-the-gpu-using-cupy">Convolution on the GPU Using CuPy<a class="anchor" aria-label="anchor" href="#convolution-on-the-gpu-using-cupy"></a>
</h1>
<p>This is part of a lesson on GPU programming, so let us use the GPU.
Although there is a physical connection - i.e. a cable - between the CPU
and the GPU, they do not share the same memory space. This image depicts
the different components of CPU and GPU and how they are connected:</p>
<figure><img src="./fig/CPU_and_GPU_separated.png" alt="CPU and GPU are two separate entities, each with its own memory" class="figure mx-auto d-block"><div class="figcaption">CPU and GPU are two separate entities, each with
its own memory</div>
</figure><p>This means that an array created from e.g. an iPython shell using
NumPy is physically located into the main memory of the host, and
therefore available for the CPU but not the GPU. It is not yet present
in GPU memory, which means that we need to copy our data, the input
image and the convolving function to the GPU, before we can execute any
code on it. In practice, we have the arrays <code>deltas</code> and
<code>gauss</code> in the host’s RAM, and we need to copy them to GPU
memory using CuPy.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> cupy <span class="im">as</span> cp</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>deltas_gpu <span class="op">=</span> cp.asarray(deltas)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>gauss_gpu <span class="op">=</span> cp.asarray(gauss)</span></code></pre>
</div>
<p>Now it is time to do the convolution on the GPU. SciPy does not offer
functions that can use the GPU, so we need to import the convolution
function from another library, called <code>cupyx</code>;
<code>cupyx.scipy</code> contains a subset of all SciPy routines. You
will see that the GPU convolution function from the <code>cupyx</code>
library looks very much like the convolution function from SciPy we used
previously. In general, NumPy and CuPy look very similar, as well as the
SciPy and <code>cupyx</code> libraries, and this is on purpose to
facilitate the use of the GPU by programmers that are already familiar
with NumPy and SciPy. Let us again record the time to execute the
convolution, so that we can compare it with the time it took on the
host.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> cupyx.scipy.signal <span class="im">import</span> convolve2d <span class="im">as</span> convolve2d_gpu</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>convolved_image_using_GPU <span class="op">=</span> convolve2d_gpu(deltas_gpu, gauss_gpu)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">7</span> <span class="op">-</span>r <span class="dv">1</span> convolved_image_using_GPU <span class="op">=</span> convolve2d_gpu(deltas_gpu, gauss_gpu)</span></code></pre>
</div>
<p>Similar to what we had previously on the host, the execution time of
the GPU convolution will depend very much on the GPU used. These are the
results using a NVIDIA Tesla T4 on Google Colab.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>98.2 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 7 loops each)</code></pre>
</div>
<p>This is a lot faster than on the host, a performance improvement, or
speedup, of 24439 times. Impressive, but is it true?</p>
</div>
<div class="section level1">
<h1 id="measuring-performance">Measuring performance<a class="anchor" aria-label="anchor" href="#measuring-performance"></a>
</h1>
<p>So far we used <code>timeit</code> to measure the performance of our
Python code, no matter if it was running on the CPU or was GPU
accelerated. However, execution on the GPU is asynchronous: the control
is given back to the Python interpreter immediately, while the GPU is
still executing the task. Because of this, we cannot use
<code>timeit</code> anymore: the timing would not be correct.</p>
<p>CuPy provides a function, <code>benchmark</code> that we can use to
measure the time it takes the GPU to execute our kernels.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(convolve2d_gpu, (deltas_gpu, gauss_gpu), n_repeat<span class="op">=</span><span class="dv">10</span>)</span></code></pre>
</div>
<p>The previous code executes <code>convolve2d_gpu</code> ten times, and
stores the execution time of each run, in seconds, inside the
<code>gpu_times</code> attribute of the variable
<code>execution_gpu</code>. We can then compute the average execution
time and print it, as shown.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>Another advantage of the <code>benchmark</code> method is that it
excludes the compile time, and warms up the GPU, so that measurements
are more stable and representative.</p>
<p>With the new measuring code in place, we can measure performance
again.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>0.020642 s</code></pre>
</div>
<p>We now have a more reasonable, but still impressive, speedup of 116
times over the host code.</p>
<div id="challenge-convolution-on-the-gpu-without-cupy" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-convolution-on-the-gpu-without-cupy" class="callout-inner">
<h3 class="callout-title">Challenge: convolution on the GPU without
CuPy<a class="anchor" aria-label="anchor" href="#challenge-convolution-on-the-gpu-without-cupy"></a>
</h3>
<div class="callout-content">
<p>Try to convolve the NumPy array <code>deltas</code> with the NumPy
array <code>gauss</code> directly on the GPU, without using CuPy arrays.
If this works, it should save us the time and effort of transferring
<code>deltas</code> and <code>gauss</code> to the GPU.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>We can directly try to use the GPU convolution function
<code>convolve2d_gpu</code> with <code>deltas</code> and
<code>gauss</code> as inputs.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>convolve2d_gpu(deltas, gauss)</span></code></pre>
</div>
<p>However, this gives a long error message ending with:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>TypeError: Unsupported type &lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
<p>It is unfortunately not possible to access NumPy arrays directly from
the GPU because they exist in the Random Access Memory (RAM) of the host
and not in GPU memory.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="validation">Validation<a class="anchor" aria-label="anchor" href="#validation"></a>
</h1>
<p>To check that we actually computed the same output on the host and
the device we can compare the two output arrays
<code>convolved_image_using_GPU</code> and
<code>convolved_image_using_CPU</code>.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>np.allclose(convolved_image_using_GPU, convolved_image_using_CPU)</span></code></pre>
</div>
<p>As you may expect, the result of the comparison is positive, and in
fact we computed the same results on the host and the device.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fu">array</span><span class="op">(</span><span class="va">True</span><span class="op">)</span></span></code></pre>
</div>
<div id="challenge-fairer-comparison-of-cpu-vs.-gpu" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-fairer-comparison-of-cpu-vs.-gpu" class="callout-inner">
<h3 class="callout-title">Challenge: fairer comparison of CPU
vs. GPU<a class="anchor" aria-label="anchor" href="#challenge-fairer-comparison-of-cpu-vs.-gpu"></a>
</h3>
<div class="callout-content">
<p>Compute again the speedup achieved using the GPU, but try to take
also into account the time spent transferring the data to the GPU and
back.</p>
<p>Hint: to copy a CuPy array back to the host (CPU), use the
<code>cp.asnumpy()</code> function.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>A convenient solution is to group both the transfers, to and from the
GPU, and the convolution into a single Python function, and then time
its execution, like in the following example.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="kw">def</span> transfer_compute_transferback():</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>    deltas_gpu <span class="op">=</span> cp.asarray(deltas)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    gauss_gpu <span class="op">=</span> cp.asarray(gauss)</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    convolved_image_using_GPU <span class="op">=</span> convolve2d_gpu(deltas_gpu, gauss_gpu)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    convolved_image_using_GPU_copied_to_host <span class="op">=</span> cp.asnumpy(convolved_image_using_GPU)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>   </span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(transfer_compute_transferback, (), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>0.035400 s</code></pre>
</div>
<p>The speedup taking into account the data transfers decreased from 116
to 67. Taking into account the necessary data transfers when computing
the speedup is a better, and more fair, way to compare performance. As a
note, because data transfers force the GPU to sync with the host, this
could also be measured with <code>timeit</code> and still provide
correct measurements.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="a-shortcut-performing-numpy-routines-on-the-gpu">A shortcut: performing NumPy routines on the GPU<a class="anchor" aria-label="anchor" href="#a-shortcut-performing-numpy-routines-on-the-gpu"></a>
</h1>
<p>We saw earlier that we cannot execute routines from the
<code>cupyx</code> library directly on NumPy arrays. In fact we need to
first transfer the data from host to device memory. Vice versa, if we
try to execute a regular SciPy routine (i.e. designed to run the CPU) on
a CuPy array, we will also encounter an error. Try the following:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>convolve2d_cpu(deltas_gpu, gauss_gpu)</span></code></pre>
</div>
<p>This results in</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>......
......
......
TypeError: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.</code></pre>
</div>
<p>So SciPy routines cannot have CuPy arrays as input. We can, however,
execute a simpler command that does not require SciPy. Instead of 2D
convolution, we can do 1D convolution. For that we can use a NumPy
routine instead of a SciPy routine. The <code>convolve</code> routine
from NumPy performs linear (1D) convolution. To generate some input for
a linear convolution, we can flatten our image from 2D to 1D (using
<code>ravel()</code>), but we also need a 1D kernel. For the latter we
will take the diagonal elements of our 2D Gaussian kernel. Try the
following three instructions for linear convolution on the CPU:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>deltas_1d <span class="op">=</span> deltas.ravel()</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>gauss_1d <span class="op">=</span> gauss.diagonal()</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> np.convolve(deltas_1d, gauss_1d)</span></code></pre>
</div>
<p>You could arrive at something similar to this timing result:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>270 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)</code></pre>
</div>
<p>We have performed a regular linear convolution using our CPU. Now let
us try something bold. We will transfer the 1D arrays to the GPU and use
the NumPy routine to do the convolution.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>deltas_1d_gpu <span class="op">=</span> cp.asarray(deltas_1d)</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>gauss_1d_gpu <span class="op">=</span> cp.asarray(gauss_1d)</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(np.convolve, (deltas_1d_gpu, gauss_1d_gpu), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>You may be surprised that we can issue these commands without error.
Contrary to SciPy routines, NumPy accepts CuPy arrays, i.e. arrays that
exist in GPU memory, as input. <a href="https://docs.cupy.dev/en/stable/user_guide/interoperability.html#numpy" class="external-link">In
the CuPy documentation</a> you can find some background on why NumPy
routines can handle CuPy arrays.</p>
<p>Also, remember when we used <code>np.allclose</code> with a NumPy and
a CuPy array as input? That worked for the same reason.</p>
<p>The linear convolution is actually performed on the GPU, which also
results in a lower execution time.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>0.014529 s</code></pre>
</div>
<p>Without much effort, we obtained a 18 times speedup.</p>
</div>
<div class="section level1">
<h1 id="a-real-world-example-image-processing-for-radio-astronomy">A real world example: image processing for radio astronomy<a class="anchor" aria-label="anchor" href="#a-real-world-example-image-processing-for-radio-astronomy"></a>
</h1>
<p>In this section, we will perform the four major steps in image
processing for radio astronomy: determination of background
characteristics, segmentation, connected component labelling and source
measurements.</p>
<div class="section level2">
<h2 id="import-the-fits-image">Import the FITS image<a class="anchor" aria-label="anchor" href="#import-the-fits-image"></a>
</h2>
<p>Start by importing a 2048² pixels image of the Galactic Center, an
image made from observations by the Indian Giant Metrewave Radio
Telescope (GMRT) at 150 MHz. The image is stored <a href="./data/GMRT_image_of_Galactic_Center.fits">in this repository</a>
as a <a href="https://en.wikipedia.org/wiki/FITS" class="external-link">FITS</a> file, and to
read it we need the <code>astropy</code> Python package.</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="im">from</span> astropy.io <span class="im">import</span> fits</span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="cf">with</span> fits.<span class="bu">open</span>(<span class="st">"GMRT_image_of_Galactic_Center.fits"</span>) <span class="im">as</span> hdul:</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>    data <span class="op">=</span> hdul[<span class="dv">0</span>].data.byteswap().newbyteorder()</span></code></pre>
</div>
<p>The latter two methods are needed to convert byte ordering from big
endian to little endian.</p>
</div>
<div class="section level2">
<h2 id="inspect-the-image">Inspect the image<a class="anchor" aria-label="anchor" href="#inspect-the-image"></a>
</h2>
<p>Let us have a look at part of this image.</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="im">from</span> matplotlib.colors <span class="im">import</span> LogNorm</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a>maxim <span class="op">=</span> data.<span class="bu">max</span>()</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a>fig <span class="op">=</span> pyl.figure(figsize<span class="op">=</span>(<span class="dv">50</span>, <span class="fl">12.5</span>))</span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a>ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a>im_plot <span class="op">=</span> ax.imshow(np.fliplr(data), cmap<span class="op">=</span>pyl.cm.gray_r, norm<span class="op">=</span>LogNorm(vmin <span class="op">=</span> maxim<span class="op">/</span><span class="dv">10</span>, vmax<span class="op">=</span>maxim<span class="op">/</span><span class="dv">100</span>))</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>pyl.colorbar(im_plot, ax<span class="op">=</span>ax)</span></code></pre>
</div>
<figure><img src="./fig/improved_image_of_GC.png" alt="Image of the Galactic Center" class="figure mx-auto d-block"><div class="figcaption">Image of the Galactic Center</div>
</figure><p>The data has been switched left to right because Right Ascension
increases to the left, so now it adheres to this astronomical
convention. We can see a few dozen sources, especially in the lower left
and upper right, which both have relatively good image quality. The band
across the image from upper left to lower right has the worst image
quality because there are many extended sources - especially the
Galactic Center itself, in the middle - which are hard to deconvolve
with the limited spacings from this observation. You can, however, see a
couple of ring-like structures. These are actually supernova shells,
i.e. the remnants from massive stars that exploded at the end of their
lifes.</p>
</div>
<div class="section level2">
<h2 id="determine-the-background-characteristics-of-the-image">Determine the background characteristics of the image<a class="anchor" aria-label="anchor" href="#determine-the-background-characteristics-of-the-image"></a>
</h2>
<p>We want to identify all the sources - meaning e.g. stars, supernova
remnants and distant galaxies - in this image and measure their
positions and fluxes. How do we separate source pixels from background
pixels? When do we know if a pixel with a high value belongs to a source
or is simply a noise peak? We assume the background noise, which is a
reflection of the limited sensitivity of the radio telescope, has a
normal distribution. The chance of having a background pixel with a
value above 5 times the standard deviation is 2.9e-7. We have 2²² =
4.2e6 pixels in our image, so the chance of catching at least one random
noise peak by setting a threshold of 5 times the standard deviation is
less than 50%. We refer to the standard deviation as <span class="math inline">\(\sigma\)</span>.</p>
<p>How do we measure the standard deviation of the background pixels?
First we need to separate them from the source pixels, based on their
values, in the sense that high pixel values more likely come from
sources. The technique we use is called <span class="math inline">\(\kappa, \sigma\)</span> clipping. First we take
all pixels and compute the standard deviation (<span class="math inline">\(\sigma\)</span>). Then we compute the median and
clip all pixels larger than <span class="math inline">\(median + 3 *
\sigma\)</span> and smaller than <span class="math inline">\(median - 3
* \sigma\)</span>. From the clipped set, we compute the median and
standard deviation again and clip again. Continue until no more pixels
are clipped. The standard deviation from this final set of pixel values
is the basis for the next step.</p>
<p>Before clipping, let us investigate some properties of our unclipped
data.</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>mean_ <span class="op">=</span> data.mean()</span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>median_ <span class="op">=</span> np.median(data)</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>stddev_ <span class="op">=</span> np.std(data)</span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>max_ <span class="op">=</span> np.amax(data)</span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"mean = </span><span class="sc">{</span>mean_<span class="sc">:.3e}</span><span class="ss">, median = </span><span class="sc">{</span>median_<span class="sc">:.3e}</span><span class="ss">, sttdev = </span><span class="sc">{</span>stddev_<span class="sc">:.3e}</span><span class="ss">, </span><span class="ch">\</span></span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a><span class="ss">maximum = </span><span class="sc">{</span>max_<span class="sc">:.3e}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>The maximum flux density is 2506 mJy/beam, coming from the Galactic
Center itself, so from the center of the image, while the overall
standard deviation is 19.9 mJy/beam:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>mean = 3.898e-04, median = 1.571e-05, sttdev = 1.993e-02, maximum = 2.506e+00</code></pre>
</div>
<p>You might observe that <span class="math inline">\(\kappa,
\sigma\)</span> clipping is a compute intense task, that is why we want
to do it on a GPU. But let’s first issue the algorithm on a CPU.</p>
<p>This is the NumPy code to do this:</p>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="co"># Flattening our 2D data first makes subsequent steps easier.</span></span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a>data_flat <span class="op">=</span> data.ravel()</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="co"># Here is a kappa, sigma clipper for the CPU</span></span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a><span class="kw">def</span> kappa_sigma_clipper(data_flat):</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a>         med <span class="op">=</span> np.median(data_flat)</span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a>         std <span class="op">=</span> np.std(data_flat)</span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>         clipped_lower <span class="op">=</span> data_flat.compress(data_flat <span class="op">&gt;</span> med <span class="op">-</span> <span class="dv">3</span> <span class="op">*</span> std)</span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a>         clipped_both <span class="op">=</span> clipped_lower.compress(clipped_lower <span class="op">&lt;</span> med <span class="op">+</span> <span class="dv">3</span> <span class="op">*</span> std)</span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a>         <span class="cf">if</span> <span class="bu">len</span>(clipped_both) <span class="op">==</span> <span class="bu">len</span>(data_flat):</span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a>             <span class="cf">break</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a>         data_flat <span class="op">=</span> clipped_both  </span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a>    <span class="cf">return</span> data_flat</span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a>data_clipped <span class="op">=</span> kappa_sigma_clipper(data_flat)</span>
<span id="cb28-16"><a href="#cb28-16" tabindex="-1"></a>timing_ks_clipping_cpu <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o kappa_sigma_clipper(data_flat)</span>
<span id="cb28-17"><a href="#cb28-17" tabindex="-1"></a>fastest_ks_clipping_cpu <span class="op">=</span> timing_ks_clipping_cpu.best</span>
<span id="cb28-18"><a href="#cb28-18" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fastest CPU ks clipping time = </span><span class="ch">\</span></span>
<span id="cb28-19"><a href="#cb28-19" tabindex="-1"></a><span class="ss">       </span><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_ks_clipping_cpu<span class="sc">:.3e}</span><span class="ss"> ms."</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>793 ms ± 17.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
Fastest CPU ks clipping time = 7.777e+02 ms.</code></pre>
</div>
<p>So that is close to 1 second to perform these computations.
Hopefully, we can speed this up using the GPU. How has <span class="math inline">\(\kappa, \sigma\)</span> clipping influenced our
statistics?</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>clipped_mean_ <span class="op">=</span> data_clipped.mean()</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>clipped_median_ <span class="op">=</span> np.median(data_clipped)</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>clipped_stddev_ <span class="op">=</span> np.std(data_clipped)</span>
<span id="cb30-4"><a href="#cb30-4" tabindex="-1"></a>clipped_max_ <span class="op">=</span> np.amax(data_clipped)</span>
<span id="cb30-5"><a href="#cb30-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"mean of clipped = </span><span class="sc">{</span>clipped_mean_<span class="sc">:.3e}</span><span class="ss">, median of clipped = </span><span class="ch">\</span></span>
<span id="cb30-6"><a href="#cb30-6" tabindex="-1"></a><span class="sc">{</span>clipped_median_<span class="sc">:.3e}</span><span class="ss"> </span><span class="ch">\n</span><span class="ss"> standard deviation of clipped = </span><span class="ch">\</span></span>
<span id="cb30-7"><a href="#cb30-7" tabindex="-1"></a><span class="sc">{</span>clipped_stddev_<span class="sc">:.3e}</span><span class="ss">, maximum of clipped = </span><span class="sc">{</span>clipped_max_<span class="sc">:.3e}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>All output statistics have become smaller which is reassuring; it
seems data_clipped contains mostly background pixels:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>mean of clipped = -1.945e-06, median of clipped = -9.796e-06 
 standard deviation of clipped = 1.334e-02, maximum of clipped = 4.000e-02</code></pre>
</div>
<div id="challenge-kappa-sigma-clipping-on-the-gpu" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-kappa-sigma-clipping-on-the-gpu" class="callout-inner">
<h3 class="callout-title">Challenge:<span class="math inline">\(\kappa,
\sigma\)</span>clipping on the GPU<a class="anchor" aria-label="anchor" href="#challenge-kappa-sigma-clipping-on-the-gpu"></a>
</h3>
<div class="callout-content">
<p>Now that you understand how the <span class="math inline">\(\kappa,
\sigma\)</span> clipping algorithm works, perform it on the GPU using
CuPy and compute the speedup. Include the data transfer to and from the
GPU in your calculation.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="kw">def</span> ks_clipper_gpu(data_flat):</span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a>    data_flat_gpu <span class="op">=</span> cp.asarray(data_flat)</span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a>    data_gpu_clipped <span class="op">=</span> kappa_sigma_clipper(data_flat_gpu)</span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>    <span class="cf">return</span> cp.asnumpy(data_gpu_clipped)</span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a>data_clipped_on_GPU <span class="op">=</span> ks_clipper_gpu(data_flat)</span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a>timing_ks_clipping_gpu <span class="op">=</span> benchmark(ks_clipper_gpu, <span class="op">\</span></span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a>                                   (data_flat, ), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a>fastest_ks_clipping_gpu <span class="op">=</span> np.amin(timing_ks_clipping_gpu.gpu_times)</span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_ks_clipping_gpu<span class="sc">:.3e}</span><span class="ss"> ms"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>6.329e+01 ms</code></pre>
</div>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>speedup_factor <span class="op">=</span> fastest_ks_clipping_cpu<span class="op">/</span>fastest_ks_clipping_gpu</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The speedup factor for ks clipping is: </span><span class="sc">{</span>speedup_factor<span class="sc">:.3e}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The speedup factor for ks clipping is: 1.232e+01</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="segment-the-image">Segment the image<a class="anchor" aria-label="anchor" href="#segment-the-image"></a>
</h2>
<p>We have seen that clipping at the <span class="math inline">\(5
\sigma\)</span> level of an image this size (2048² pixels) will yield a
chance of less than 50% that from all the sources we detect at least one
will be a noise peak. So let us set the threshold at <span class="math inline">\(5 \sigma\)</span> and segment it. First check that
we find the same standard deviation from our clipper on the GPU:</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>stddev_gpu_ <span class="op">=</span> np.std(data_clipped_on_GPU)</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"standard deviation of background_noise = </span><span class="sc">{</span>stddev_gpu_<span class="sc">:.4f}</span><span class="ss"> Jy/beam"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>standard deviation of background_noise = 0.0133 Jy/beam</code></pre>
</div>
<p>With the standard deviation computed we apply the <span class="math inline">\(5 \sigma\)</span> threshold to the image.</p>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a>threshold <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> stddev_gpu_</span>
<span id="cb38-2"><a href="#cb38-2" tabindex="-1"></a>segmented_image <span class="op">=</span> np.where(data <span class="op">&gt;</span> threshold, <span class="dv">1</span>,  <span class="dv">0</span>)</span>
<span id="cb38-3"><a href="#cb38-3" tabindex="-1"></a>timing_segmentation_CPU <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o np.where(data <span class="op">&gt;</span> threshold, <span class="dv">1</span>,  <span class="dv">0</span>)</span>
<span id="cb38-4"><a href="#cb38-4" tabindex="-1"></a>fastest_segmentation_CPU <span class="op">=</span> timing_segmentation_CPU.best </span>
<span id="cb38-5"><a href="#cb38-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fastest CPU segmentation time = </span><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_segmentation_CPU<span class="sc">:.3e}</span><span class="ss"> </span><span class="ch">\</span></span>
<span id="cb38-6"><a href="#cb38-6" tabindex="-1"></a><span class="ss">ms."</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>6.41 ms ± 55.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
Fastest CPU segmentation time = 6.294e+00 ms.</code></pre>
</div>
</div>
<div class="section level2">
<h2 id="labelling-of-the-segmented-data">Labelling of the segmented data<a class="anchor" aria-label="anchor" href="#labelling-of-the-segmented-data"></a>
</h2>
<p>This is called connected component labelling (CCL). It will replace
pixel values in the segmented image - just consisting of zeros and ones
- of the first connected group of pixels with the value 1 - so nothing
changed, but just for that first group - the pixel values in the second
group of connected pixels will all be 2, the third connected group of
pixels will all have the value 3 etc.</p>
<p>This is a CPU code for connected component labelling.</p>
<div class="codewrapper sourceCode" id="cb40">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="im">from</span> scipy.ndimage <span class="im">import</span> label <span class="im">as</span> label_cpu</span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a>labelled_image <span class="op">=</span> np.empty(data.shape)</span>
<span id="cb40-3"><a href="#cb40-3" tabindex="-1"></a>number_of_sources_in_image <span class="op">=</span> label_cpu(segmented_image, output <span class="op">=</span> labelled_image)</span>
<span id="cb40-4"><a href="#cb40-4" tabindex="-1"></a>sigma_unicode <span class="op">=</span> <span class="st">"</span><span class="ch">\u03C3</span><span class="st">"</span></span>
<span id="cb40-5"><a href="#cb40-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The number of sources in the image at the 5</span><span class="sc">{</span>sigma_unicode<span class="sc">}</span><span class="ss"> level is </span><span class="ch">\</span></span>
<span id="cb40-6"><a href="#cb40-6" tabindex="-1"></a><span class="sc">{</span>number_of_sources_in_image<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb40-7"><a href="#cb40-7" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" tabindex="-1"></a>timing_CCL_CPU <span class="op">=</span> <span class="op">%</span>timeit <span class="op">-</span>o label_cpu(segmented_image, output <span class="op">=</span> labelled_image)</span>
<span id="cb40-9"><a href="#cb40-9" tabindex="-1"></a>fastest_CCL_CPU <span class="op">=</span> timing_CCL_CPU.best</span>
<span id="cb40-10"><a href="#cb40-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fastest CPU CCL time = </span><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_CCL_CPU<span class="sc">:.3e}</span><span class="ss"> ms."</span>)</span></code></pre>
</div>
<p>This gives, on my machine:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The number of sources in the image at the 5σ level is 185.
26.3 ms ± 965 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
Fastest CPU CCL time = 2.546e+01 ms.</code></pre>
</div>
<p>Let us not just accept the answer, but also do a sanity check. What
are the values in the labelled image?</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"These are all the pixel values we can find in the labelled image: </span><span class="ch">\</span></span>
<span id="cb42-2"><a href="#cb42-2" tabindex="-1"></a><span class="sc">{</span>np<span class="sc">.</span>unique(labelled_image)<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>This should show the following output:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>These are all the pixel values we can find in the labelled image: [  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.
  14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.
  28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.
  42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.
  56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.
  70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.
  84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.
  98.  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111.
 112. 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125.
 126. 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139.
 140. 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153.
 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167.
 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181.
 182. 183. 184. 185.]</code></pre>
</div>
</div>
</div>
<div class="section level1">
<h1 id="source-measurements">Source measurements<a class="anchor" aria-label="anchor" href="#source-measurements"></a>
</h1>
<p>We are ready for the final step. We have been given observing time to
make this beautiful image of the Galactic Center, we have determined its
background statistics, we have separated actual cosmic sources from
noise and now we want to measure these cosmic sources. What are their
positions and what are their flux densities?</p>
<p>Again, the algorithms from <code>scipy.ndimage</code> help us to
determine these quantities. This is the CPU code for measuring our
sources.</p>
<div class="codewrapper sourceCode" id="cb44">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="im">from</span> scipy.ndimage <span class="im">import</span> center_of_mass <span class="im">as</span> com_cpu</span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a><span class="im">from</span> scipy.ndimage <span class="im">import</span> sum_labels <span class="im">as</span> sl_cpu</span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a>all_positions <span class="op">=</span> com_cpu(data, labelled_image, <span class="op">\</span></span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a>                               <span class="bu">range</span>(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb44-5"><a href="#cb44-5" tabindex="-1"></a>all_integrated_fluxes <span class="op">=</span> sl_cpu(data, labelled_image, <span class="op">\</span></span>
<span id="cb44-6"><a href="#cb44-6" tabindex="-1"></a>                               <span class="bu">range</span>(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb44-7"><a href="#cb44-7" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" tabindex="-1"></a><span class="bu">print</span> (<span class="ss">f'These are the ten highest integrated fluxes of the sources in my </span><span class="ch">\n\</span></span>
<span id="cb44-9"><a href="#cb44-9" tabindex="-1"></a><span class="ss">image: </span><span class="sc">{</span>np<span class="sc">.</span>sort(all_integrated_fluxes)[<span class="op">-</span><span class="dv">10</span>:]<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p>which gives the Galactic Center as the most luminous source, which
makes sense when we look at our image.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>These are the ten highest integrated fluxes of the sources in my image: [ 38.90615184  41.91485894  43.02203498  47.30590784  51.23707351
  58.07289425  68.85673917  70.31223921  95.16443585 363.58937774]</code></pre>
</div>
<p>Now we can try to measure the execution times for both algorithms,
like this:</p>
<div class="codewrapper sourceCode" id="cb46">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="op">%%</span>timeit <span class="op">-</span>o</span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a>all_positions <span class="op">=</span> com_cpu(data, labelled_image, <span class="op">\</span></span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a>                               <span class="bu">range</span>(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a>all_integrated_fluxes <span class="op">=</span> sl_cpu(data, labelled_image, <span class="op">\</span></span>
<span id="cb46-5"><a href="#cb46-5" tabindex="-1"></a>                               <span class="bu">range</span>(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span></code></pre>
</div>
<p>Which yields, on my machine:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>797 ms ± 9.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

&lt;TimeitResult : 797 ms ± 9.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)&gt;</code></pre>
</div>
<p>To collect the result from that timing in our next cell block, we
need a trick that uses the <code>_</code> variable.</p>
<div class="codewrapper sourceCode" id="cb48">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a>timing_source_measurements_CPU <span class="op">=</span> _</span>
<span id="cb48-2"><a href="#cb48-2" tabindex="-1"></a>fastest_source_measurements_CPU <span class="op">=</span> timing_source_measurements_CPU.best</span>
<span id="cb48-3"><a href="#cb48-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Fastest CPU set of source measurements = </span><span class="ch">\</span></span>
<span id="cb48-4"><a href="#cb48-4" tabindex="-1"></a><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_source_measurements_CPU<span class="sc">:.3e}</span><span class="ss"> ms."</span>)</span></code></pre>
</div>
<p>Which yields</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Fastest CPU set of source measurements = 7.838e+02 ms.</code></pre>
</div>
<div id="challenge-putting-it-all-together" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-putting-it-all-together" class="callout-inner">
<h3 class="callout-title">Challenge: putting it all together<a class="anchor" aria-label="anchor" href="#challenge-putting-it-all-together"></a>
</h3>
<div class="callout-content">
<p>Combine the first two steps of image processing for astronomy,
i.e. determining background characteristics e.g. through <span class="math inline">\(\kappa, \sigma\)</span> clipping and segmentation
into a single function, that works for both CPU and GPU. Next, write a
function for connected component labelling and source measurements on
the GPU and calculate the overall speedup factor for the combined four
steps of image processing in astronomy on the GPU relative to the CPU.
Finally, verify your output by comparing with the previous output, using
the CPU.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<div class="codewrapper sourceCode" id="cb50">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a><span class="kw">def</span> first_two_steps_for_both_CPU_and_GPU(data):</span>
<span id="cb50-2"><a href="#cb50-2" tabindex="-1"></a>    data_flat <span class="op">=</span> data.ravel()</span>
<span id="cb50-3"><a href="#cb50-3" tabindex="-1"></a>    data_clipped <span class="op">=</span> kappa_sigma_clipper(data_flat)</span>
<span id="cb50-4"><a href="#cb50-4" tabindex="-1"></a>    stddev_ <span class="op">=</span> np.std(data_clipped)</span>
<span id="cb50-5"><a href="#cb50-5" tabindex="-1"></a>    threshold <span class="op">=</span> <span class="dv">5</span> <span class="op">*</span> stddev_</span>
<span id="cb50-6"><a href="#cb50-6" tabindex="-1"></a>    segmented_image <span class="op">=</span> np.where(data <span class="op">&gt;</span> threshold, <span class="dv">1</span>,  <span class="dv">0</span>)</span>
<span id="cb50-7"><a href="#cb50-7" tabindex="-1"></a>    <span class="cf">return</span> segmented_image</span>
<span id="cb50-8"><a href="#cb50-8" tabindex="-1"></a></span>
<span id="cb50-9"><a href="#cb50-9" tabindex="-1"></a><span class="kw">def</span> ccl_and_source_measurements_on_CPU(data_CPU, segmented_image_CPU):</span>
<span id="cb50-10"><a href="#cb50-10" tabindex="-1"></a>    labelled_image_CPU <span class="op">=</span> np.empty(data_CPU.shape)</span>
<span id="cb50-11"><a href="#cb50-11" tabindex="-1"></a>    number_of_sources_in_image <span class="op">=</span> label_cpu(segmented_image_CPU, </span>
<span id="cb50-12"><a href="#cb50-12" tabindex="-1"></a>                                       output<span class="op">=</span> labelled_image_CPU)</span>
<span id="cb50-13"><a href="#cb50-13" tabindex="-1"></a>    all_positions <span class="op">=</span> com_cpu(data_CPU, labelled_image_CPU, </span>
<span id="cb50-14"><a href="#cb50-14" tabindex="-1"></a>                            np.arange(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb50-15"><a href="#cb50-15" tabindex="-1"></a>    all_fluxes <span class="op">=</span> sl_cpu(data_CPU, labelled_image_CPU, </span>
<span id="cb50-16"><a href="#cb50-16" tabindex="-1"></a>                            np.arange(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb50-17"><a href="#cb50-17" tabindex="-1"></a>    <span class="cf">return</span> np.array(all_positions), np.array(all_fluxes)</span>
<span id="cb50-18"><a href="#cb50-18" tabindex="-1"></a></span>
<span id="cb50-19"><a href="#cb50-19" tabindex="-1"></a>CPU_output <span class="op">=</span> ccl_and_source_measurements_on_CPU(data, <span class="op">\</span></span>
<span id="cb50-20"><a href="#cb50-20" tabindex="-1"></a>                 first_two_steps_for_both_CPU_and_GPU(data))</span>
<span id="cb50-21"><a href="#cb50-21" tabindex="-1"></a></span>
<span id="cb50-22"><a href="#cb50-22" tabindex="-1"></a>timing_complete_processing_CPU <span class="op">=</span>  <span class="op">\</span></span>
<span id="cb50-23"><a href="#cb50-23" tabindex="-1"></a>    benchmark(ccl_and_source_measurements_on_CPU, (data, <span class="op">\</span></span>
<span id="cb50-24"><a href="#cb50-24" tabindex="-1"></a>       first_two_steps_for_both_CPU_and_GPU(data)), <span class="op">\</span></span>
<span id="cb50-25"><a href="#cb50-25" tabindex="-1"></a>       n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb50-26"><a href="#cb50-26" tabindex="-1"></a></span>
<span id="cb50-27"><a href="#cb50-27" tabindex="-1"></a>fastest_complete_processing_CPU <span class="op">=</span> <span class="op">\</span></span>
<span id="cb50-28"><a href="#cb50-28" tabindex="-1"></a>    np.amin(timing_complete_processing_CPU.cpu_times)</span>
<span id="cb50-29"><a href="#cb50-29" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The four steps of image processing for astronomy take </span><span class="ch">\</span></span>
<span id="cb50-31"><a href="#cb50-31" tabindex="-1"></a><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_complete_processing_CPU<span class="sc">:.3e}</span><span class="ss"> ms</span><span class="ch">\n</span><span class="ss"> on our CPU."</span>)</span>
<span id="cb50-32"><a href="#cb50-32" tabindex="-1"></a></span>
<span id="cb50-33"><a href="#cb50-33" tabindex="-1"></a><span class="im">from</span> cupyx.scipy.ndimage <span class="im">import</span> label <span class="im">as</span> label_gpu</span>
<span id="cb50-34"><a href="#cb50-34" tabindex="-1"></a><span class="im">from</span> cupyx.scipy.ndimage <span class="im">import</span> center_of_mass <span class="im">as</span> com_gpu</span>
<span id="cb50-35"><a href="#cb50-35" tabindex="-1"></a><span class="im">from</span> cupyx.scipy.ndimage <span class="im">import</span> sum_labels <span class="im">as</span> sl_gpu</span>
<span id="cb50-36"><a href="#cb50-36" tabindex="-1"></a></span>
<span id="cb50-37"><a href="#cb50-37" tabindex="-1"></a><span class="kw">def</span> ccl_and_source_measurements_on_GPU(data_GPU, segmented_image_GPU):</span>
<span id="cb50-38"><a href="#cb50-38" tabindex="-1"></a>    labelled_image_GPU <span class="op">=</span> cp.empty(data_GPU.shape)</span>
<span id="cb50-39"><a href="#cb50-39" tabindex="-1"></a>    number_of_sources_in_image <span class="op">=</span> label_gpu(segmented_image_GPU, </span>
<span id="cb50-40"><a href="#cb50-40" tabindex="-1"></a>                                           output<span class="op">=</span> labelled_image_GPU)</span>
<span id="cb50-41"><a href="#cb50-41" tabindex="-1"></a>    all_positions <span class="op">=</span> com_gpu(data_GPU, labelled_image_GPU, </span>
<span id="cb50-42"><a href="#cb50-42" tabindex="-1"></a>                            cp.arange(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb50-43"><a href="#cb50-43" tabindex="-1"></a>    all_fluxes <span class="op">=</span> sl_gpu(data_GPU, labelled_image_GPU, </span>
<span id="cb50-44"><a href="#cb50-44" tabindex="-1"></a>                            cp.arange(<span class="dv">1</span>, number_of_sources_in_image<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb50-45"><a href="#cb50-45" tabindex="-1"></a>    <span class="co"># This seems redundant, but we want to return ndarrays (Numpy)</span></span>
<span id="cb50-46"><a href="#cb50-46" tabindex="-1"></a>    <span class="co"># and what we have are lists. These first have to be converted to</span></span>
<span id="cb50-47"><a href="#cb50-47" tabindex="-1"></a>    <span class="co"># Cupy arrays before they can be converted to Numpy arrays.</span></span>
<span id="cb50-48"><a href="#cb50-48" tabindex="-1"></a>    <span class="cf">return</span> cp.asnumpy(cp.asarray(all_positions)), <span class="op">\</span></span>
<span id="cb50-49"><a href="#cb50-49" tabindex="-1"></a>           cp.asnumpy(cp.asarray(all_fluxes))</span>
<span id="cb50-50"><a href="#cb50-50" tabindex="-1"></a></span>
<span id="cb50-51"><a href="#cb50-51" tabindex="-1"></a>GPU_output <span class="op">=</span> ccl_and_source_measurements_on_GPU(cp.asarray(data), <span class="op">\</span></span>
<span id="cb50-52"><a href="#cb50-52" tabindex="-1"></a>                 first_two_steps_for_both_CPU_and_GPU(cp.asarray(data)))</span>
<span id="cb50-53"><a href="#cb50-53" tabindex="-1"></a></span>
<span id="cb50-54"><a href="#cb50-54" tabindex="-1"></a>timing_complete_processing_GPU <span class="op">=</span>  <span class="op">\</span></span>
<span id="cb50-55"><a href="#cb50-55" tabindex="-1"></a>    benchmark(ccl_and_source_measurements_on_GPU, (cp.asarray(data), <span class="op">\</span></span>
<span id="cb50-56"><a href="#cb50-56" tabindex="-1"></a>       first_two_steps_for_both_CPU_and_GPU(cp.asarray(data))), <span class="op">\</span></span>
<span id="cb50-57"><a href="#cb50-57" tabindex="-1"></a>       n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb50-58"><a href="#cb50-58" tabindex="-1"></a></span>
<span id="cb50-59"><a href="#cb50-59" tabindex="-1"></a>fastest_complete_processing_GPU <span class="op">=</span> <span class="op">\</span></span>
<span id="cb50-60"><a href="#cb50-60" tabindex="-1"></a>    np.amin(timing_complete_processing_GPU.gpu_times)</span>
<span id="cb50-61"><a href="#cb50-61" tabindex="-1"></a></span>
<span id="cb50-62"><a href="#cb50-62" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The four steps of image processing for astronomy take </span><span class="ch">\</span></span>
<span id="cb50-63"><a href="#cb50-63" tabindex="-1"></a><span class="sc">{</span><span class="dv">1000</span> <span class="op">*</span> fastest_complete_processing_GPU<span class="sc">:.3e}</span><span class="ss"> ms</span><span class="ch">\n</span><span class="ss"> on our GPU."</span>)</span>
<span id="cb50-64"><a href="#cb50-64" tabindex="-1"></a></span>
<span id="cb50-65"><a href="#cb50-65" tabindex="-1"></a>overall_speedup_factor <span class="op">=</span> fastest_complete_processing_CPU<span class="op">/</span> <span class="op">\</span></span>
<span id="cb50-66"><a href="#cb50-66" tabindex="-1"></a>                         fastest_complete_processing_GPU</span>
<span id="cb50-67"><a href="#cb50-67" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This means that the overall speedup factor GPU vs CPU equals: </span><span class="ch">\</span></span>
<span id="cb50-68"><a href="#cb50-68" tabindex="-1"></a><span class="sc">{</span>overall_speedup_factor<span class="sc">:.3e}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb50-69"><a href="#cb50-69" tabindex="-1"></a></span>
<span id="cb50-70"><a href="#cb50-70" tabindex="-1"></a>all_positions_agree <span class="op">=</span> np.allclose(CPU_output[<span class="dv">0</span>], GPU_output[<span class="dv">0</span>])</span>
<span id="cb50-71"><a href="#cb50-71" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The CPU and GPU positions agree: </span><span class="sc">{</span>all_positions_agree<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb50-72"><a href="#cb50-72" tabindex="-1"></a></span>
<span id="cb50-73"><a href="#cb50-73" tabindex="-1"></a>all_fluxes_agree <span class="op">=</span> np.allclose(CPU_output[<span class="dv">1</span>], GPU_output[<span class="dv">1</span>])</span>
<span id="cb50-74"><a href="#cb50-74" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The CPU and GPU fluxes agree: </span><span class="sc">{</span>all_positions_agree<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>The four steps of image processing for astronomy take 1.060e+03 ms 
 on our CPU.
The four steps of image processing for astronomy take 5.770e+01 ms 
 on our GPU.
This means that the overall speedup factor GPU vs CPU equals: 1.838e+01

The CPU and GPU positions agree: True

The CPU and GPU fluxes agree: True</code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“CuPy provides GPU accelerated version of many NumPy and Scipy
functions.”</li>
<li>“Always have CPU and GPU versions of your code so that you can
compare performance, as well as validate your code.”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-numba"><p>Content from <a href="numba.html">Accelerate your Python code with Numba</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/numba.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“How can I run my own Python functions on the GPU?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Learn how to use Numba decorators to improve the performance of
your Python code.”</li>
<li>“Run your first application on the GPU.”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="using-numba-to-execute-python-code-on-the-gpu">Using Numba to execute Python code on the GPU<a class="anchor" aria-label="anchor" href="#using-numba-to-execute-python-code-on-the-gpu"></a>
</h1>
<p><a href="http://numba.pydata.org/" class="external-link">Numba</a> is a Python library that
“translates Python functions to optimized machine code at runtime using
the industry-standard LLVM compiler library”. You might want to try it
to speed up your code on a CPU. However, Numba <a href="https://numba.pydata.org/numba-doc/latest/cuda/overview.html" class="external-link">can
also translate a subset of the Python language into CUDA</a>, which is
what we will be using here. So the idea is that we can do what we are
used to, i.e. write Python code and still benefit from the speed that
GPUs offer us.</p>
<p>We want to compute all <a href="https://en.wikipedia.org/wiki/Prime_number" class="external-link">prime numbers</a> -
i.e. numbers that have only 1 or themselves as exact divisors - between
1 and 10000 on the CPU and see if we can speed it up, by deploying a
similar algorithm on a GPU. This is code that you can find on many
websites. Small variations are possible, but it will look something like
this:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">def</span> find_all_primes_cpu(upper):</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>    all_prime_numbers <span class="op">=</span> []</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, upper):</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>        prime <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, (num <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>            <span class="cf">if</span> (num <span class="op">%</span> i) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>                prime <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>        <span class="cf">if</span> prime:</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>            all_prime_numbers.append(num)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>    <span class="cf">return</span> all_prime_numbers</span></code></pre>
</div>
<p>Calling <code>find_all_primes_cpu(10_000)</code> will return all
prime numbers between 1 and 10000 as a list. Let us time it:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>r <span class="dv">1</span> find_all_primes_cpu(<span class="dv">10_000</span>)</span></code></pre>
</div>
<p>You will probably find that <code>find_all_primes_cpu</code> takes
several hundreds of milliseconds to complete:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>176 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)</code></pre>
</div>
<p>As a quick sidestep, add Numba’s JIT (Just in Time compilation)
decorator to the <code>find_all_primes_cpu</code> function. You can
either add it to the function definition or to the call, so either in
this way:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> jit</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="at">@jit</span>(nopython<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="kw">def</span> find_all_primes_cpu(upper):</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    all_prime_numbers <span class="op">=</span> []</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, upper):</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>        prime <span class="op">=</span> <span class="va">True</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, (num <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>            <span class="cf">if</span> (num <span class="op">%</span> i) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>                prime <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>        <span class="cf">if</span> prime:</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>            all_prime_numbers.append(num)</span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a>    <span class="cf">return</span> all_prime_numbers</span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>r <span class="dv">1</span> find_all_primes_cpu(<span class="dv">10_000</span>)</span></code></pre>
</div>
<p>or in this way:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> jit</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>upper <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>r <span class="dv">1</span> jit(nopython<span class="op">=</span><span class="va">True</span>)(find_all_primes_cpu)(upper)</span></code></pre>
</div>
<p>which can give you a timing result similar to this:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>69.5 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)</code></pre>
</div>
<p>So twice as fast, by using a simple decorator. The speedup is much
larger for <code>upper = 100_000</code>, but that takes a little too
much waiting time for this course. Despite the
<code>jit(nopython=True)</code> decorator the computation is still
performed on the CPU. Let us move the computation to the GPU. There are
a number of ways to achieve this, one of them is the usage of the
<code>jit(device=True)</code> decorator, but it depends very much on the
nature of the computation. Let us write our first GPU kernel which
checks if a number is a prime, using the <code>cuda.jit</code>
decorator, so different from the <code>jit</code> decorator for CPU
computations. It is essentially the inner loop of
<code>find_all_primes_cpu</code>:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">from</span> numba <span class="im">import</span> cuda</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="at">@cuda.jit</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="kw">def</span> check_prime_gpu_kernel(num, result):</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>   result[<span class="dv">0</span>] <span class="op">=</span>  num</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>   <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, (num <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a>       <span class="cf">if</span> (num <span class="op">%</span> i) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>           result[<span class="dv">0</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>           <span class="cf">break</span></span></code></pre>
</div>
<p>A number of things are worth noting. CUDA kernels do not return
anything, so you have to supply for an array to be modified. All
arguments have to be arrays, if you work with scalars, make them arrays
of length one. This is the case here, because we check if a single
number is a prime or not. Let us see if this works:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>result <span class="op">=</span> np.zeros((<span class="dv">1</span>), np.int32)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>check_prime_gpu_kernel[<span class="dv">1</span>, <span class="dv">1</span>](<span class="dv">11</span>, result)</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>])</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>check_prime_gpu_kernel[<span class="dv">1</span>, <span class="dv">1</span>](<span class="dv">12</span>, result)</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="bu">print</span>(result[<span class="dv">0</span>])</span></code></pre>
</div>
<p>If we have not made any mistake, the first call should return “11”,
because 11 is a prime number, while the second call should return “0”
because 12 is not a prime:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">11</span></span>
<span><span class="fl">0</span></span></code></pre>
</div>
<p>Note the extra arguments in square brackets - <code>[1, 1]</code> -
that are added to the call of <code>check_prime_gpu_kernel</code>: these
indicate the number of threads we want to run on the GPU. While this is
an important argument, we will explain it later and for now we can keep
using <code>1</code>.</p>
<div id="challenge-compute-prime-numbers" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-compute-prime-numbers" class="callout-inner">
<h3 class="callout-title">Challenge: compute prime numbers<a class="anchor" aria-label="anchor" href="#challenge-compute-prime-numbers"></a>
</h3>
<div class="callout-content">
<p>Write a new function <code>find_all_primes_cpu_and_gpu</code> that
uses <code>check_prime_gpu_kernel</code> instead of the inner loop of
<code>find_all_primes_cpu</code>. How long does this new function take
to find all primes up to 10000?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>One possible implementation of this function is the following
one.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">def</span> find_all_primes_cpu_and_gpu(upper):</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    all_prime_numbers <span class="op">=</span> []</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, upper):</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>        result <span class="op">=</span> np.zeros((<span class="dv">1</span>), np.int32)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>        check_prime_gpu_kernel[<span class="dv">1</span>,<span class="dv">1</span>](num, result)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>        <span class="cf">if</span> result[<span class="dv">0</span>] <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>            all_prime_numbers.append(num)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>    <span class="cf">return</span> all_prime_numbers</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>   </span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>r <span class="dv">1</span> find_all_primes_cpu_and_gpu(<span class="dv">10_000</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>6.21 s ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)</code></pre>
</div>
<p>As you may have noticed, <code>find_all_primes_cpu_and_gpu</code> is
much slower than the original <code>find_all_primes_cpu</code>. The
reason is that the overhead of calling the GPU, and transferring data to
and from it, for each number of the sequence is too large. To be
efficient the GPU needs enough work to keep all of its cores busy.</p>
</div>
</div>
</div>
</div>
<p>Let us give the GPU a work load large enough to compensate for the
overhead of data transfers to and from the GPU. For this example of
computing primes we can best use the <code>vectorize</code> decorator
for a new <code>check_prime_gpu</code> function that takes an array as
input instead of <code>upper</code> in order to increase the work load.
This is the array we have to use as input for our new
<code>check_prime_gpu</code> function, instead of upper, a single
integer:</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>numbers <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">10_000</span>, dtype<span class="op">=</span>np.int32)</span></code></pre>
</div>
<p>So that input to the new <code>check_prime_gpu</code> function is
simply the array of numbers we need to check for primes.
<code>check_prime_gpu</code> looks similar to
<code>check_prime_gpu_kernel</code>, but it is not a kernel, so it can
return values:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> numba <span class="im">as</span> nb</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="at">@nb.vectorize</span>([<span class="st">'int32(int32)'</span>], target<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="kw">def</span> check_prime_gpu(num):</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, (num <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>       <span class="cf">if</span> (num <span class="op">%</span> i) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>           <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>    <span class="cf">return</span> num</span></code></pre>
</div>
<p>where we have added the <code>vectorize</code> decorator from Numba.
The argument of <code>check_prime_gpu</code> seems to be defined as a
scalar (single integer in this case), but the <code>vectorize</code>
decorator will allow us to use an array as input. That array should
consist of 4B (bytes) or 32b (bit) integers, indicated by
<code>(int32)</code>. The return array will also consist of 32b
integers, with zeros for the non-primes. The nonzero values are the
primes.</p>
<p>Let us run it and record the elapsed time:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">10</span> <span class="op">-</span>r <span class="dv">1</span> check_prime_gpu(numbers)</span></code></pre>
</div>
<p>which should show you a significant speedup:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>5.9 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 10 loops each)</code></pre>
</div>
<p>This amounts to a speedup of our code of a factor 11 compared to the
<code>jit(nopython=True)</code> decorated code on the CPU.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“Numba can be used to run your own Python functions on the
GPU.”</li>
<li>“Functions may need to be changed to run correctly on a GPU.”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-gpu_introduction"><p>Content from <a href="gpu_introduction.html">A Better Look at the GPU</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/gpu_introduction.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“How does a GPU work?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Understand how the GPU is organized.”</li>
<li>“Understand the building blocks of the general GPU programming
model.”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>So far we have learned how to replace calls to NumPy and SciPy
functions to equivalent ones running on the GPU using CuPy, and how to
run some of our own Python functions on the GPU using Numba. This was
possible even without much knowledge of how a GPU works. In fact, the
only thing we mentioned in previous episodes about the GPU is that it is
a device specialized in running parallel workloads, and that it is its
own system, connected to our main memory and CPU by some kind of
bus.</p>
<figure><img src="./fig/CPU_and_GPU_separated.png" alt="The connection between CPU and GPU" class="figure mx-auto d-block"><div class="figcaption">The connection between CPU and GPU</div>
</figure><p>However, before moving to a programming language designed especially
for GPUs, we need to introduce some concepts that will be useful to
understand the next episodes.</p>
<div class="section level1">
<h1 id="the-gpu-a-high-level-view-at-the-hardware">The GPU, a High Level View at the Hardware<a class="anchor" aria-label="anchor" href="#the-gpu-a-high-level-view-at-the-hardware"></a>
</h1>
<p>We can see the GPU like a collection of processors, sharing some
common memory, akin to a traditional multi-processor system. Each
processor executes code independently of the others, and internally it
has tens to hundreds of cores, and some private memory space; in some
GPUs the different processors can even execute different programs. The
cores are often grouped in groups, and each group executes the same
code, instruction by instruction, in the same order and at the same
time. All cores have access to the processor’s private memory space.</p>
</div>
<div class="section level1">
<h1 id="how-programs-are-executed">How Programs are Executed<a class="anchor" aria-label="anchor" href="#how-programs-are-executed"></a>
</h1>
<p>Let us assume we are sending our code to the GPU for execution; how
is the code being executed by the different processors? First of all, we
need to know that each processor will execute the same code. As an
example, we can look back at some code that we executed on the GPU using
Numba, like the following snippet.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numba <span class="im">as</span> nb</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="at">@nb.vectorize</span>([<span class="st">'int32(int32)'</span>], target<span class="op">=</span><span class="st">'cuda'</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="kw">def</span> check_prime_gpu(num):</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, (num <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>       <span class="cf">if</span> (num <span class="op">%</span> i) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>           <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    <span class="cf">return</span> num</span></code></pre>
</div>
<p>We did not need to write a different <code>check_prime_gpu</code>
function for each processor, or core, on the GPU; actually, we have no
idea how many processors and cores are available on the GPU we just used
to execute this code!</p>
<p>So we can imagine that each processors receives its copy of the
<code>check_prime_gpu</code> function, and executes it independently of
the other processors. We also know that by executing the following
Python snippet, we are telling the GPU to execute our function on all
numbers between 0 and 100000.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>check_prime_gpu(np.arange(<span class="dv">0</span>, <span class="dv">10_000</span>, dtype<span class="op">=</span>np.int32))</span></code></pre>
</div>
<p>So each processor will get a copy of the code, and one subset of the
numbers between 0 and 10000. If we assume that our GPU has 4 processors,
each of them will get around 2500 numbers to process; the processing of
these numbers will be split among the various cores that the processor
has. Again, let us assume that each processor has 8 cores, divided in 2
groups of 4 cores. Therefore, the 2500 numbers to process will be
divided inside the processors in sets of 4 elements, and these sets will
be scheduled for execution on the 2 groups of cores that each processor
has available. While the processors cannot communicate with each other,
the cores of the same processor can; however, there is no communication
in our example code.</p>
<p>While so far in the lesson we had no control over the way in which
the computation is mapped to the GPU for execution, this is something
that we will address soon.</p>
</div>
<div class="section level1">
<h1 id="different-memories">Different Memories<a class="anchor" aria-label="anchor" href="#different-memories"></a>
</h1>
<p>Another detail that we need to understand is that GPUs have different
memories. We have a main memory that is available to all processors on
the GPU; this memory, as we already know, is often physically separate
from the CPU memory, but copies to and from are possible. Using this
memory we can send data to the GPU, and copy results back to the CPU.
This memory is not coherent, meaning that there is no guarantee that
code running on one GPU processor will see the results of code running
on a different GPU processor.</p>
<p>Internally, each processor has its own memory. This memory is faster
than the GPU main memory, but smaller in size, and it is also coherent,
although we need to wait for all cores to finish their memory operations
before the results produced by some cores are available to all other
cores in the same processor. Therefore, this memory can be used for
communication among cores.</p>
<p>Finally, each core has also a very small, but very fast, memory, that
is used mainly to store the operands of the instructions executed by
each core. This memory is private, and cannot generally be used for
communication.</p>
</div>
<div class="section level1">
<h1 id="additional-material">Additional Material<a class="anchor" aria-label="anchor" href="#additional-material"></a>
</h1>
<p>A short, but at the same time detailed, introduction to GPU hardware
and programming model can be found in the following video, extracted
from the University of Utah’s undergraduate course on Computer
Organization and presented by Rajeev Balasubramonian.</p>
<p><a href="https://www.youtube.com/watch?v=FcS_kQOIykU" class="external-link"><img src="https://img.youtube.com/vi/FcS_kQOIykU/0.jpg" alt="Screenshot of the YouTube video showing a slide" class="figure"></a></p>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-first_program"><p>Content from <a href="first_program.html">Your First GPU Kernel</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/first_program.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“How can I parallelize a Python application on a GPU?”</li>
<li>“How to write a GPU program?”</li>
<li>“What is CUDA?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Recognize possible data parallelism in Python code”</li>
<li>“Understand the structure of a CUDA program”</li>
<li>“Execute a CUDA program in Python using CuPy”</li>
<li>“Measure the execution time of a CUDA kernel with CuPy”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="summing-two-vectors-in-python">Summing Two Vectors in Python<a class="anchor" aria-label="anchor" href="#summing-two-vectors-in-python"></a>
</h1>
<p>We start by introducing a program that, given two input vectors of
the same size, stores the sum of the corresponding elements of the two
input vectors into a third one.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">def</span> vector_add(A, B, C, size):</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, size):</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>        C[item] <span class="op">=</span> A[item] <span class="op">+</span> B[item]</span></code></pre>
</div>
<p>One of the characteristics of this program is that each iteration of
the <code>for</code> loop is independent from the other iterations. In
other words, we could reorder the iterations and still produce the same
output, or even compute each iteration in parallel or on a different
device, and still come up with the same output. These are the kind of
programs that we would call <em>naturally parallel</em>, and they are
perfect candidates for being executed on a GPU.</p>
</div>
<div class="section level1">
<h1 id="summing-two-vectors-in-cuda">Summing Two Vectors in CUDA<a class="anchor" aria-label="anchor" href="#summing-two-vectors-in-cuda"></a>
</h1>
<p>While we could just use CuPy to run something equivalent to our
<code>vector_add</code> on a GPU, our goal is to learn how to write code
that can be executed by GPUs, therefore we now begin learning CUDA.</p>
<p>The CUDA-C language is a GPU programming language and API developed
by NVIDIA. It is mostly equivalent to C/C++, with some special keywords,
built-in variables, and functions.</p>
<p>We begin our introduction to CUDA by writing a small kernel, i.e. a
GPU program, that computes the same function that we just described in
Python.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>We are aware that CUDA is a proprietary solution, and that there are
open-source alternatives such as OpenCL. However, CUDA is the most used
platform for GPU programming and therefore we decided to use it for our
teaching material.</p>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="running-code-on-the-gpu-with-cupy">Running Code on the GPU with CuPy<a class="anchor" aria-label="anchor" href="#running-code-on-the-gpu-with-cupy"></a>
</h1>
<p>Before delving deeper into the meaning of all lines of code, and
before starting to understand how CUDA works, let us execute the code on
a GPU and check if it is correct or not. To compile the code and manage
the GPU in Python we are going to use the interface provided by
CuPy.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># size of the vectors</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co"># allocating and populating the vectors</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="co"># CUDA vector_add</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>vector_add_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="vs">    int item = threadIdx.x;</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="vs">    C[item] = A[item] + B[item];</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(vector_add_cuda_code, <span class="st">"vector_add"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a>vector_add_gpu((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size))</span></code></pre>
</div>
<p>And to be sure that the CUDA code does exactly what we want, we can
execute our sequential Python code and compare the results.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>a_cpu <span class="op">=</span> cupy.asnumpy(a_gpu)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>b_cpu <span class="op">=</span> cupy.asnumpy(b_gpu)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>c_cpu <span class="op">=</span> np.zeros(size, dtype<span class="op">=</span>np.float32)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>vector_add(a_cpu, b_cpu, c_cpu, size)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="co"># test</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="cf">if</span> np.allclose(c_cpu, c_gpu):</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Correct results!</code></pre>
</div>
</div>
<div class="section level1">
<h1 id="understanding-the-cuda-code">Understanding the CUDA Code<a class="anchor" aria-label="anchor" href="#understanding-the-cuda-code"></a>
</h1>
<p>We can now move back to the CUDA code and analyze it line by line to
highlight the differences between CUDA-C and standard C.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span></code></pre>
</div>
<p>This is the definition of our CUDA <code>vector_add</code> function.
The <code>__global__</code> keyword is an execution space identifier,
and it is specific to CUDA. What this keyword means is that the defined
function will be able to run on the GPU, but can also be called from the
host (in our case the Python interpreter running on the CPU). All of our
kernel definitions will be preceded by this keyword.</p>
<p>Other execution space identifiers in CUDA-C are
<code>__host__</code>, and <code>__device__</code>. Functions annotated
with the <code>__host__</code> identifier will run on the host, and be
only callable from the host, while functions annotated with the
<code>__device__</code> identifier will run on the GPU, but can only be
called from the GPU itself. We are not going to use these identifiers as
often as <code>__global__</code>.</p>
<p>The following table offers a recapitulation of the keywords we just
introduced.</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Keyword</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>__global__</code></td>
<td>the function is visible to the host and the GPU, and runs on the
GPU</td>
</tr>
<tr class="even">
<td><code>__host__</code></td>
<td>the function is visible only to the host, and runs on the host</td>
</tr>
<tr class="odd">
<td><code>__device__</code></td>
<td>the function is visible only to the GPU, and runs on the GPU</td>
</tr>
</tbody>
</table>
<p>The following is the part of the code in which we do the actual
work.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="dt">int</span> item <span class="op">=</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span></code></pre>
</div>
<p>As you may see, it looks similar to the innermost loop of our
<code>vector_add</code> Python function, with the main difference being
in how the value of the <code>item</code> variable is evaluated.</p>
<p>In fact, while in Python the content of <code>item</code> is the
result of the <code>range</code> function, in CUDA we are reading a
special variable, i.e. <code>threadIdx</code>, containing a triplet that
indicates the id of a thread inside a three-dimensional CUDA block. In
this particular case we are working on a one dimensional vector, and
therefore only interested in the first dimension, that is stored in the
<code>x</code> field of this variable.</p>
<div id="challenge-loose-threads" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-loose-threads" class="callout-inner">
<h3 class="callout-title">Challenge: loose threads<a class="anchor" aria-label="anchor" href="#challenge-loose-threads"></a>
</h3>
<div class="callout-content">
<p>We know enough now to pause for a moment and do a little exercise.
Assume that in our <code>vector_add</code> kernel we replace the
following line:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="dt">int</span> item <span class="op">=</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span></code></pre>
</div>
<p>With this other line of code:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="dt">int</span> item <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span></code></pre>
</div>
<p>What will the result of this change be?</p>
<ol style="list-style-type: decimal">
<li>Nothing changes</li>
<li>Only the first thread is working</li>
<li>Only <code>C[1]</code> is written</li>
<li>All elements of <code>C</code> are zero</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>The correct answer is number 3, only the element <code>C[1]</code> is
written, and we do not even know by which thread!</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="computing-hierarchy-in-cuda">Computing Hierarchy in CUDA<a class="anchor" aria-label="anchor" href="#computing-hierarchy-in-cuda"></a>
</h1>
<p>In the previous example we had a small vector of size 1024, and each
of the 1024 threads we generated was working on one of the element.</p>
<p>What would happen if we changed the size of the vector to a larger
number, such as 2048? We modify the value of the variable size and try
again.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="co"># size of the vectors</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="co"># allocating and populating the vectors</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="co"># CUDA vector_add</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(<span class="vs">r'''</span></span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a><span class="vs">    int item = threadIdx.x;</span></span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a><span class="vs">    C[item] = A[item] + B[item];</span></span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a><span class="vs">'''</span>, <span class="st">"vector_add"</span>)</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a>vector_add_gpu((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size))</span></code></pre>
</div>
<p>This is how the output should look like when running the code in a
Jupyter Notebook:</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>---------------------------------------------------------------------------

CUDADriverError                           Traceback (most recent call last)

&lt;ipython-input-4-a26bc8acad2fin &lt;module&gt;()
     19 ''', "vector_add")
     20 
---21 vector_add_gpu((1, 1, 1), (size, 1, 1), (a_gpu, b_gpu, c_gpu, size))
     22 
     23 print(c_gpu)

cupy/core/raw.pyx in cupy.core.raw.RawKernel.__call__()

cupy/cuda/function.pyx in cupy.cuda.function.Function.__call__()

cupy/cuda/function.pyx in cupy.cuda.function._launch()

cupy_backends/cuda/api/driver.pyx in cupy_backends.cuda.api.driver.launchKernel()

cupy_backends/cuda/api/driver.pyx in cupy_backends.cuda.api.driver.check_status()

CUDADriverError: CUDA_ERROR_INVALID_VALUE: invalid argument</code></pre>
</div>
<p>The reason for this error is that most GPUs will not allow us to
execute a block composed of more than 1024 threads. If we look at the
parameters of our functions we see that the first two parameters are two
triplets.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>vector_add_gpu((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size))</span></code></pre>
</div>
<p>The first triplet specifies the size of the CUDA
<strong>grid</strong>, while the second triplet specifies the size of
the CUDA <strong>block</strong>. The grid is a three-dimensional
structure in the CUDA programming model and it represent the
organization of a whole kernel execution. A grid is made of one or more
independent blocks, and in the case of our previous snippet of code we
have a grid composed by a single block <code>(1, 1, 1)</code>. The size
of this block is specified by the second triplet, in our case
<code>(size, 1, 1)</code>. While blocks are independent of each other,
the thread composing a block are not completely independent, they share
resources and can also communicate with each other.</p>
<p>To go back to our example, we can modify che grid specification from
<code>(1, 1, 1)</code> to <code>(2, 1, 1)</code>, and the block
specification from <code>(size, 1, 1)</code> to
<code>(size // 2, 1, 1)</code>. If we run the code again, we should now
get the expected output.</p>
<p>We already introduced the special variable <code>threadIdx</code>
when introducing the <code>vector_add</code> CUDA code, and we said it
contains a triplet specifying the coordinates of a thread in a thread
block. CUDA has other variables that are important to understand the
coordinates of each thread and block in the overall structure of the
computation.</p>
<p>These special variables are <code>blockDim</code>,
<code>blockIdx</code>, and <code>gridDim</code>, and they are all
triplets. The triplet contained in <code>blockDim</code> represents the
size of the calling thread’s block in three dimensions. While the
content of <code>threadIdx</code> is different for each thread in the
same block, the content of <code>blockDim</code> is the same because the
size of the block is the same for all threads. The coordinates of a
block in the computational grid are contained in <code>blockIdx</code>,
therefore the content of this variable will be the same for all threads
in the same block, but different for threads in different blocks.
Finally, <code>gridDim</code> contains the size of the grid in three
dimensions, and it is again the same for all threads.</p>
<p>The following table offers a recapitulation of the keywords we just
introduced.</p>
<table class="table">
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Keyword</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>threadIdx</code></td>
<td>the ID of a thread in a block</td>
</tr>
<tr class="even">
<td><code>blockDim</code></td>
<td>the size of a block, i.e. the number of threads per dimension</td>
</tr>
<tr class="odd">
<td><code>blockIdx</code></td>
<td>the ID of a block in the grid</td>
</tr>
<tr class="even">
<td><code>gridDim</code></td>
<td>the size of the grid, i.e. the number of blocks per dimension</td>
</tr>
</tbody>
</table>
<div id="challenge-hidden-variables" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-hidden-variables" class="callout-inner">
<h3 class="callout-title">Challenge: hidden variables<a class="anchor" aria-label="anchor" href="#challenge-hidden-variables"></a>
</h3>
<div class="callout-content">
<p>Given the following snippet of code:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>vector_add_gpu((<span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size))</span></code></pre>
</div>
<p>What is the content of the <code>blockDim</code> and
<code>gridDim</code> variables inside the CUDA <code>vector_add</code>
kernel?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>The content of <code>blockDim</code> is <code>(512, 1, 1)</code> and
the content of <code>gridDim</code> is <code>(4, 1, 1)</code>, for all
threads.</p>
</div>
</div>
</div>
</div>
<p>What happens if we run the code that we just modified to work on an
vector of 2048 elements, and compare the results with our CPU
version?</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># size of the vectors</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="co"># allocating and populating the vectors</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>a_cpu <span class="op">=</span> cupy.asnumpy(a_gpu)</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>b_cpu <span class="op">=</span> cupy.asnumpy(b_gpu)</span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>c_cpu <span class="op">=</span> np.zeros(size, dtype<span class="op">=</span>np.float32)</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a><span class="co"># CPU code</span></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a><span class="kw">def</span> vector_add(A, B, C, size):</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, size):</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>        C[item] <span class="op">=</span> A[item] <span class="op">+</span> B[item]</span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a><span class="co"># CUDA vector_add</span></span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(<span class="vs">r'''</span></span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb14-20"><a href="#cb14-20" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb14-21"><a href="#cb14-21" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb14-22"><a href="#cb14-22" tabindex="-1"></a><span class="vs">    int item = threadIdx.x;</span></span>
<span id="cb14-23"><a href="#cb14-23" tabindex="-1"></a><span class="vs">    C[item] = A[item] + B[item];</span></span>
<span id="cb14-24"><a href="#cb14-24" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb14-25"><a href="#cb14-25" tabindex="-1"></a><span class="vs">'''</span>, <span class="st">"vector_add"</span>)</span>
<span id="cb14-26"><a href="#cb14-26" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" tabindex="-1"></a><span class="co"># execute the code</span></span>
<span id="cb14-28"><a href="#cb14-28" tabindex="-1"></a>vector_add_gpu((<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size))</span>
<span id="cb14-29"><a href="#cb14-29" tabindex="-1"></a>vector_add(a_cpu, b_cpu, c_cpu, size)</span>
<span id="cb14-30"><a href="#cb14-30" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" tabindex="-1"></a><span class="co"># test</span></span>
<span id="cb14-32"><a href="#cb14-32" tabindex="-1"></a><span class="cf">if</span> np.allclose(c_cpu, c_gpu):</span>
<span id="cb14-33"><a href="#cb14-33" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb14-34"><a href="#cb14-34" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb14-35"><a href="#cb14-35" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Wrong results!</code></pre>
</div>
<p>The results are wrong! In fact, while we increased the number of
threads we launch, we did not modify the kernel code to compute the
correct results using the new builtin variables we just introduced.</p>
<div id="challenge-scaling-up" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-scaling-up" class="callout-inner">
<h3 class="callout-title">Challenge: scaling up<a class="anchor" aria-label="anchor" href="#challenge-scaling-up"></a>
</h3>
<div class="callout-content">
<p>In the following code, fill in the blank to work with vectors that
are larger than the largest CUDA block (i.e. 1024).</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> ______________<span class="op">;</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>   C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>The correct answer is
<code>(blockIdx.x * blockDim.x) + threadIdx.x</code>. The following code
is the complete <code>vector_add</code> that can work with vectors
larger than 1024 elements.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>   C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="vectors-of-arbitrary-size">Vectors of Arbitrary Size<a class="anchor" aria-label="anchor" href="#vectors-of-arbitrary-size"></a>
</h1>
<p>So far we have worked with a number of threads that is the same as
the elements in the vector. However, in a real world scenario we may
have to process vectors of arbitrary size, and to do this we need to
modify both the kernel and the way it is launched.</p>
<div id="challenge-more-work-than-necessary" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-more-work-than-necessary" class="callout-inner">
<h3 class="callout-title">Challenge: more work than necessary<a class="anchor" aria-label="anchor" href="#challenge-more-work-than-necessary"></a>
</h3>
<div class="callout-content">
<p>We modified the <code>vector_add</code> kernel to include a check for
the size of the vector, so that we only compute elements that are within
the vector boundaries. However the code is not correct as it is written
now. Can you rewrite the code to make it work?</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>      <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>   C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>The correct way to modify the <code>vector_add</code> to work on
vectors of arbitrary size is to first compute the coordinates of each
thread, and then perform the sum only on elements that are within the
vector boundaries, as shown in the following snippet of code.</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>To test our changes we can modify the <code>size</code> of the
vectors from 2048 to 10000, and execute the code again.</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>---------------------------------------------------------------------------

CUDADriverError                           Traceback (most recent call last)

&lt;ipython-input-20-00d938215d28in &lt;module&gt;()
     31 
     32 # Execute the code
---33 vector_add_gpu((2, 1, 1), (size // 2, 1, 1), (a_gpu, b_gpu, c_gpu, size))
     34 vector_add(a_cpu, b_cpu, c_cpu, size)
     35 

cupy/core/raw.pyx in cupy.core.raw.RawKernel.__call__()

cupy/cuda/function.pyx in cupy.cuda.function.Function.__call__()

cupy/cuda/function.pyx in cupy.cuda.function._launch()

cupy/cuda/driver.pyx in cupy.cuda.driver.launchKernel()

cupy/cuda/driver.pyx in cupy.cuda.driver.check_status()

CUDADriverError: CUDA_ERROR_INVALID_VALUE: invalid argument</code></pre>
</div>
<p>This error is telling us that CUDA cannot launch a block with
<code>size // 2</code> threads, because the maximum amount of threads in
a kernel is 1024 and we are requesting 5000 threads.</p>
<p>What we need to do is to make grid and block more flexible, so that
they can adapt to vectors of arbitrary size. To do that, we can replace
the Python code to call <code>vector_add_gpu</code> with the following
code.</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> <span class="dv">1024</span>)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>block_size <span class="op">=</span> (<span class="dv">1024</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>vector_add_gpu(grid_size, block_size, (a_gpu, b_gpu, c_gpu, size))</span></code></pre>
</div>
<p>With these changes we always have blocks composed of 1024 threads,
but we adapt the number of blocks so that we always have enough to
threads to compute all elements in the vector. If we want to be able to
easily modify the number of threads per block, we can even rewrite the
code like the following:</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>vector_add_gpu(grid_size, block_size, (a_gpu, b_gpu, c_gpu, size))</span></code></pre>
</div>
<p>So putting this all together in a full snippet we can execute the
code again.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>vector_add_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a><span class="vs">   int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a><span class="vs">   if ( item &lt; size )</span></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="vs">   {</span></span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a><span class="vs">      C[item] = A[item] + B[item];</span></span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a><span class="vs">   }</span></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(vector_add_cuda_code, <span class="st">"vector_add"</span>)</span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a></span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a>vector_add_gpu(grid_size, block_size, (a_gpu, b_gpu, c_gpu, size))</span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a><span class="cf">if</span> np.allclose(c_cpu, c_gpu):</span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb23-22"><a href="#cb23-22" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb23-23"><a href="#cb23-23" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Correct results!</code></pre>
</div>
<div id="challenge-compute-prime-numbers-with-cuda" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-compute-prime-numbers-with-cuda" class="callout-inner">
<h3 class="callout-title">Challenge: compute prime numbers with
CUDA<a class="anchor" aria-label="anchor" href="#challenge-compute-prime-numbers-with-cuda"></a>
</h3>
<div class="callout-content">
<p>Given the following Python code, similar to what we have seen in the
previous episode about Numba, write the missing CUDA kernel that
computes all the prime numbers up to a certain upper bound.</p>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a><span class="co"># CPU version</span></span>
<span id="cb25-7"><a href="#cb25-7" tabindex="-1"></a><span class="kw">def</span> all_primes_to(upper : <span class="bu">int</span>, prime_list : <span class="bu">list</span>):</span>
<span id="cb25-8"><a href="#cb25-8" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, upper):</span>
<span id="cb25-9"><a href="#cb25-9" tabindex="-1"></a>        prime <span class="op">=</span> <span class="va">True</span></span>
<span id="cb25-10"><a href="#cb25-10" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>, (num <span class="op">//</span> <span class="dv">2</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb25-11"><a href="#cb25-11" tabindex="-1"></a>            <span class="cf">if</span> (num <span class="op">%</span> i) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb25-12"><a href="#cb25-12" tabindex="-1"></a>                prime <span class="op">=</span> <span class="va">False</span></span>
<span id="cb25-13"><a href="#cb25-13" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb25-14"><a href="#cb25-14" tabindex="-1"></a>        <span class="cf">if</span> prime:</span>
<span id="cb25-15"><a href="#cb25-15" tabindex="-1"></a>            prime_list[num] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb25-16"><a href="#cb25-16" tabindex="-1"></a></span>
<span id="cb25-17"><a href="#cb25-17" tabindex="-1"></a>upper_bound <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb25-18"><a href="#cb25-18" tabindex="-1"></a>all_primes_cpu <span class="op">=</span> np.zeros(upper_bound, dtype<span class="op">=</span>np.int32)</span>
<span id="cb25-19"><a href="#cb25-19" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" tabindex="-1"></a><span class="co"># GPU version</span></span>
<span id="cb25-21"><a href="#cb25-21" tabindex="-1"></a>check_prime_gpu_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb25-22"><a href="#cb25-22" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb25-23"><a href="#cb25-23" tabindex="-1"></a><span class="vs">__global__ void all_primes_to(int size, int * const all_prime_numbers)</span></span>
<span id="cb25-24"><a href="#cb25-24" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb25-25"><a href="#cb25-25" tabindex="-1"></a><span class="vs">   for ( int number = 0; number &lt; size; number++ )</span></span>
<span id="cb25-26"><a href="#cb25-26" tabindex="-1"></a><span class="vs">   {</span></span>
<span id="cb25-27"><a href="#cb25-27" tabindex="-1"></a><span class="vs">       int result = 1;</span></span>
<span id="cb25-28"><a href="#cb25-28" tabindex="-1"></a><span class="vs">       for ( int factor = 2; factor &lt;= number / 2; factor++ )</span></span>
<span id="cb25-29"><a href="#cb25-29" tabindex="-1"></a><span class="vs">       {</span></span>
<span id="cb25-30"><a href="#cb25-30" tabindex="-1"></a><span class="vs">           if ( number </span><span class="sc">% f</span><span class="vs">actor == 0 )</span></span>
<span id="cb25-31"><a href="#cb25-31" tabindex="-1"></a><span class="vs">           {</span></span>
<span id="cb25-32"><a href="#cb25-32" tabindex="-1"></a><span class="vs">               result = 0;</span></span>
<span id="cb25-33"><a href="#cb25-33" tabindex="-1"></a><span class="vs">               break;</span></span>
<span id="cb25-34"><a href="#cb25-34" tabindex="-1"></a><span class="vs">           }</span></span>
<span id="cb25-35"><a href="#cb25-35" tabindex="-1"></a><span class="vs">       }</span></span>
<span id="cb25-36"><a href="#cb25-36" tabindex="-1"></a><span class="vs">&gt;</span></span>
<span id="cb25-37"><a href="#cb25-37" tabindex="-1"></a><span class="vs">       all_prime_numbers[number] = result;</span></span>
<span id="cb25-38"><a href="#cb25-38" tabindex="-1"></a><span class="vs">   }</span></span>
<span id="cb25-39"><a href="#cb25-39" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb25-40"><a href="#cb25-40" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb25-41"><a href="#cb25-41" tabindex="-1"></a><span class="co"># Allocate memory</span></span>
<span id="cb25-42"><a href="#cb25-42" tabindex="-1"></a>all_primes_gpu <span class="op">=</span> cupy.zeros(upper_bound, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb25-43"><a href="#cb25-43" tabindex="-1"></a></span>
<span id="cb25-44"><a href="#cb25-44" tabindex="-1"></a><span class="co"># Setup the grid</span></span>
<span id="cb25-45"><a href="#cb25-45" tabindex="-1"></a>all_primes_to_gpu <span class="op">=</span> cupy.RawKernel(check_prime_gpu_code, <span class="st">"all_primes_to"</span>)</span>
<span id="cb25-46"><a href="#cb25-46" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(upper_bound <span class="op">/</span> <span class="dv">1024</span>)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb25-47"><a href="#cb25-47" tabindex="-1"></a>block_size <span class="op">=</span> (<span class="dv">1024</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb25-48"><a href="#cb25-48" tabindex="-1"></a></span>
<span id="cb25-49"><a href="#cb25-49" tabindex="-1"></a><span class="co"># Benchmark and test</span></span>
<span id="cb25-50"><a href="#cb25-50" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> all_primes_to(upper_bound, all_primes_cpu)</span>
<span id="cb25-51"><a href="#cb25-51" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(all_primes_to_gpu, (grid_size, block_size, (upper_bound, all_primes_gpu)), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb25-52"><a href="#cb25-52" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb25-53"><a href="#cb25-53" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span>
<span id="cb25-54"><a href="#cb25-54" tabindex="-1"></a><span class="op">&gt;</span></span>
<span id="cb25-55"><a href="#cb25-55" tabindex="-1"></a><span class="cf">if</span> np.allclose(all_primes_cpu, all_primes_gpu):</span>
<span id="cb25-56"><a href="#cb25-56" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb25-57"><a href="#cb25-57" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb25-58"><a href="#cb25-58" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span></code></pre>
</div>
<p>There is no need to modify anything in the code, except the body of
the CUDA <code>all_primes_to</code> inside the
<code>check_prime_gpu_code</code> string, as we did in the examples so
far.</p>
<p>Be aware that the provided CUDA code is a direct port of the Python
code, and therefore very slow. If you want to test it, user a lower
value for <code>upper_bound</code>.</p>
</div>
</div>
</div>
<div id="accordionSolution5" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution5" aria-expanded="false" aria-controls="collapseSolution5">
  <h4 class="accordion-header" id="headingSolution5">Show me the solution</h4>
</button>
<div id="collapseSolution5" class="accordion-collapse collapse" data-bs-parent="#accordionSolution5" aria-labelledby="headingSolution5">
<div class="accordion-body">
<p>One possible solution for the CUDA kernel is provided in the
following code.</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>__global__ <span class="dt">void</span> all_primes_to<span class="op">(</span><span class="dt">int</span> size<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> <span class="dt">const</span> all_prime_numbers<span class="op">)</span></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a>    <span class="dt">int</span> number <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>    <span class="dt">int</span> result <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span> number <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb26-9"><a href="#cb26-9" tabindex="-1"></a>    <span class="op">{</span></span>
<span id="cb26-10"><a href="#cb26-10" tabindex="-1"></a>        <span class="cf">for</span> <span class="op">(</span> <span class="dt">int</span> factor <span class="op">=</span> <span class="dv">2</span><span class="op">;</span> factor <span class="op">&lt;=</span> number <span class="op">/</span> <span class="dv">2</span><span class="op">;</span> factor<span class="op">++</span> <span class="op">)</span></span>
<span id="cb26-11"><a href="#cb26-11" tabindex="-1"></a>        <span class="op">{</span></span>
<span id="cb26-12"><a href="#cb26-12" tabindex="-1"></a>            <span class="cf">if</span> <span class="op">(</span> number <span class="op">%</span> factor <span class="op">==</span> <span class="dv">0</span> <span class="op">)</span></span>
<span id="cb26-13"><a href="#cb26-13" tabindex="-1"></a>            <span class="op">{</span></span>
<span id="cb26-14"><a href="#cb26-14" tabindex="-1"></a>                result <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb26-15"><a href="#cb26-15" tabindex="-1"></a>                <span class="cf">break</span><span class="op">;</span></span>
<span id="cb26-16"><a href="#cb26-16" tabindex="-1"></a>            <span class="op">}</span></span>
<span id="cb26-17"><a href="#cb26-17" tabindex="-1"></a>        <span class="op">}</span></span>
<span id="cb26-18"><a href="#cb26-18" tabindex="-1"></a></span>
<span id="cb26-19"><a href="#cb26-19" tabindex="-1"></a>        all_prime_numbers<span class="op">[</span>number<span class="op">]</span> <span class="op">=</span> result<span class="op">;</span></span>
<span id="cb26-20"><a href="#cb26-20" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb26-21"><a href="#cb26-21" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The outermost loop in Python is replaced by having each thread
testing for primeness a different number of the sequence. Having one
number assigned to each thread via its ID, the kernel implements the
innermost loop the same way it is implemented in Python.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“Precede your kernel definition with the <code>__global__</code>
keyword”</li>
<li>“Use built-in variables <code>threadIdx</code>,
<code>blockIdx</code>, <code>gridDim</code> and <code>blockDim</code> to
identify each thread”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-global_local_memory"><p>Content from <a href="global_local_memory.html">Registers, Global, and Local Memory</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/global_local_memory.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“What are registers?”</li>
<li>“How to share data between host and GPU?”</li>
<li>“Which memory is accessible to threads and thread blocks?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Understanding the difference between registers and device
memory”</li>
<li>“Understanding the difference between local and global memory”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Now that we know how to write a CUDA kernel to run code on the GPU,
and how to use the Python interface provided by CuPy to execute it, it
is time to look at the different memory spaces in the CUDA programming
model.</p>
<div class="section level1">
<h1 id="registers">Registers<a class="anchor" aria-label="anchor" href="#registers"></a>
</h1>
<p>Registers are fast on-chip memories that are used to store operands
for the operations executed by the computing cores.</p>
<p>Did we encounter registers in the <code>vector_add</code> code used
in the previous episode? Yes we did! The variable <code>item</code> is,
in fact, stored in a register for at least part, if not all, of a
thread’s execution. In general all scalar variables defined in CUDA code
are stored in registers.</p>
<p>Registers are local to a thread, and each thread has exclusive access
to its own registers: values in registers cannot be accessed by other
threads, even from the same block, and are not available for the host.
Registers are also not permanent, therefore data stored in registers is
only available during the execution of a thread.</p>
<div id="challenge-how-many-registers-are-we-using" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-how-many-registers-are-we-using" class="callout-inner">
<h3 class="callout-title">Challenge: how many registers are we
using?<a class="anchor" aria-label="anchor" href="#challenge-how-many-registers-are-we-using"></a>
</h3>
<div class="callout-content">
<p>Can you guess how many registers are we using in the following
<code>vector_add</code> code?</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>   </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>In general, it is not possible to exactly know how many registers the
compiler will use without examining the output generated by the compiler
itself. However, we can roughly estimate the amount of necessary
registers based on the variables used. We most probably need one
register to store the variable <code>item</code>, two registers to store
the content of <code>A[item]</code> and <code>B[item]</code>, and one
additional register to store the sum <code>A[item] + B[item]</code>. So
the number of registers that <code>vector_add</code> probably uses is
4.</p>
</div>
</div>
</div>
</div>
<p>If we want to make registers use more explicit in the
<code>vector_add</code> code, we can try to rewrite it in a slightly
different, but equivalent, way.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>   <span class="dt">float</span> temp_a<span class="op">,</span> temp_b<span class="op">,</span> temp_c<span class="op">;</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>       temp_a <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>       temp_b <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>       temp_c <span class="op">=</span> temp_a <span class="op">+</span> temp_b<span class="op">;</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>       C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp_c<span class="op">;</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>In this new version of <code>vector_add</code> we explicitly declare
three <code>float</code> variables to store the values loaded from
memory and the sum of our input items, making the estimation of used
registers more obvious.</p>
<p>This it totally unnecessary in the case of our example, because the
compiler will determine on its own the right amount of registers to
allocate per thread, and what to store in them. However, explicit
register usage can be important for reusing items already loaded from
memory.</p>
<div id="callout1" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout1"></a>
</h3>
<div class="callout-content">
<p>Registers are the fastest memory on the GPU, so using them to
increase data reuse is an important performance optimization. We will
look at some examples of manually using registers to improve performance
in future episodes.</p>
</div>
</div>
</div>
<p>Small CUDA arrays, which size is known at compile time, will also be
allocated in registers by the compiler. We can rewrite the previous
version of <code>vector_add</code> to work with an array of
registers.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>   <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>       temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a>       temp<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a>       temp<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a>       C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>Once again, this is not something that we would normally do, and it
is provided only as an example of how to work with arrays of
registers.</p>
</div>
<div class="section level1">
<h1 id="global-memory">Global Memory<a class="anchor" aria-label="anchor" href="#global-memory"></a>
</h1>
<p>Global memory can be considered the main memory space of the GPU in
CUDA. It is allocated, and managed, by the host, and it is accessible to
both the host and the GPU, and for this reason the global memory space
can be used to exchange data between the two. It is the largest memory
space available, and therefore it can contain much more data than
registers, but it is also slower to access. This memory space does not
require any special memory space identifier.</p>
<div id="challenge-identify-when-global-memory-is-used" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-identify-when-global-memory-is-used" class="callout-inner">
<h3 class="callout-title">Challenge: identify when global memory is
used<a class="anchor" aria-label="anchor" href="#challenge-identify-when-global-memory-is-used"></a>
</h3>
<div class="callout-content">
<p>Observe the code of the following <code>vector_add</code> and
identify where global memory is used.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>   </span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>The vectors <code>A</code>, <code>B</code>, and <code>C</code> are
stored in global memory.</p>
</div>
</div>
</div>
</div>
<p>Memory allocated on the host, and passed as a parameter to a kernel,
is by default allocated in global memory.</p>
<p>Global memory is accessible by all threads, from all thread blocks.
This means that a thread can read and write any value in global
memory.</p>
<div id="callout2" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Callout<a class="anchor" aria-label="anchor" href="#callout2"></a>
</h3>
<div class="callout-content">
<p>While global memory is visible to all threads, remember that global
memory is not coherent, and changes made by one thread block may not be
available to other thread blocks during the kernel execution. However,
all memory operations are finalized when the kernel terminates.</p>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="local-memory">Local Memory<a class="anchor" aria-label="anchor" href="#local-memory"></a>
</h1>
<p>Memory can also be statically allocated from within a kernel, and
according to the CUDA programming model such memory will not be global
but <em>local</em> memory. Local memory is only visible, and therefore
accessible, by the thread allocating it. So all threads executing a
kernel will have their own privately allocated local memory.</p>
<div id="challenge-use-local-memory" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-use-local-memory" class="callout-inner">
<h3 class="callout-title">Challenge: use local memory<a class="anchor" aria-label="anchor" href="#challenge-use-local-memory"></a>
</h3>
<div class="callout-content">
<p>Modify the following of <code>vector_add</code> so that intermediate
data products are stored in local memory, and only the final result is
saved into global memory.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>   </span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>Hint: have a look at the example using an array of registers, but
find a way to use a variable and not a constant for the size.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>We need to pass the size of the local array as a new parameter to the
kernel, because if we just specified <code>3</code> in the code, the
compiler would allocate registers and not local memory.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> local_memory_size<span class="op">)</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>   <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>   <span class="dt">float</span> local_memory<span class="op">[</span>local_memory_size<span class="op">];</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>   </span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>   <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>   <span class="op">{</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>      local_memory<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>      local_memory<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>      local_memory<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> local_memory<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">+</span> local_memory<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> local_memory<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>   <span class="op">}</span></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The host code could be modified adding one line and changing the way
the kernel is called.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>local_memory_size <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>vector_add_gpu((<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size, local_memory_size))</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>Local memory is not not a particularly fast memory, and in fact it
has similar throughput and latency of global memory, but it is much
larger than registers. As an example, local memory is automatically used
by the CUDA compiler to store spilled registers, i.e. to temporarily
store variables that cannot be kept in registers anymore because there
is not enough space in the register file, but that will be used again in
the future and so cannot be erased.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“Registers can be used to locally store data and avoid repeated
memory operations”</li>
<li>“Global memory is the main memory space and it is used to share data
between host and GPU”</li>
<li>“Local memory is a particular type of memory that can be used to
store data that does not fit in registers and is private to a
thread”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-shared_memory_and_synchronization"><p>Content from <a href="shared_memory_and_synchronization.html">Shared Memory and Synchronization</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/shared_memory_and_synchronization.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“Is there a way to share data between threads of a same block?”</li>
<li>“Can threads inside a block wait for other threads?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Learn how to share data between threads”</li>
<li>“Learn how to synchronize threads”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>So far we looked at how to use CUDA to accelerate the computation,
but a common pattern in all the examples we encountered so far is that
threads worked in isolation. While having different threads perform the
same operation on different data is a good pattern for working with
GPUs, there are cases in which threads need to communicate. This
communication may be necessary because of the way the algorithm we are
trying to implement works, or it may derive from a performance goal we
are trying to achieve.</p>
<div class="section level1">
<h1 id="shared-memory">Shared Memory<a class="anchor" aria-label="anchor" href="#shared-memory"></a>
</h1>
<p>Shared memory is a CUDA memory space that is shared by all threads in
a thread block. In this case <em>shared</em> means that all threads in a
thread block can write and read to block-allocated shared memory, and
all changes to this memory will be eventually available to all threads
in the block.</p>
<p>To allocate an array in shared memory we need to preface the
definition with the identifier <code>__shared__</code>.</p>
<div id="challenge-use-of-shared-memory" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-use-of-shared-memory" class="callout-inner">
<h3 class="callout-title">Challenge: use of shared memory<a class="anchor" aria-label="anchor" href="#challenge-use-of-shared-memory"></a>
</h3>
<div class="callout-content">
<p>Modify the following code to allocate the <code>temp</code> array in
shared memory.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>  <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>To use shared memory for the <code>temp</code> array add the
identifier <code>__shared__</code> to its definition, like in the
following code.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>  <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>  __shared__ <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span><span class="op">];</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="op">&gt;</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>      temp<span class="op">[</span><span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span><span class="dv">1</span><span class="op">];</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span><span class="dv">2</span><span class="op">];</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
</div>
<p>While syntactically correct, the previous example is functionally
wrong. The reason is that the <code>temp</code> array is not anymore
private to the thread allocating it, but it is now shared by the whole
thread block.</p>
<div id="challenge-what-is-the-result-of-the-previous-code-block" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-what-is-the-result-of-the-previous-code-block" class="callout-inner">
<h3 class="callout-title">Challenge: what is the result of the previous
code block?<a class="anchor" aria-label="anchor" href="#challenge-what-is-the-result-of-the-previous-code-block"></a>
</h3>
<div class="callout-content">
<p>The previous code example is functionally wrong. Can you guess what
the result of its execution will be?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>The result is non deterministic, and definitely not the same as the
previous versions of <code>vector_add</code>. Threads will overwrite
each other temporary values,and there will be no guarantee on which
value is visible by each thread.</p>
</div>
</div>
</div>
</div>
<p>To fix the previous kernel we should allocate enough shared memory
for each thread to store three values, so that each thread has its own
section of the shared memory array to work with.</p>
<p>To allocate enough memory we need to replace the constant 3 in
<code>__shared__ float temp[3]</code> with something else. If we know
that each thread block has 1024 threads, we can write something like the
following:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>__shared__ <span class="dt">float</span> temp<span class="op">[</span><span class="dv">3</span> <span class="op">*</span> <span class="dv">1024</span><span class="op">];</span></span></code></pre>
</div>
<p>But we know by experience that having constants in the code is not a
scalable and maintainable solution. The problem is that we need to have
a constant value if we want to declare a shared memory array, because
the compiler needs to know how much memory to allocate.</p>
<p>A solution to this problem is to not specify the size of the array,
and allocate the memory somewhere else.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">extern</span> __shared__ <span class="dt">float</span> temp<span class="op">[];</span></span></code></pre>
</div>
<p>And then use CuPy to instruct CUDA about how much shared memory, in
bytes, each thread block needs. This can be done by adding the named
parameter <code>shared_mem</code> to the kernel call.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>vector_add_gpu((<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (a_gpu, b_gpu, c_gpu, size), shared_mem<span class="op">=</span>((size <span class="op">//</span> <span class="dv">2</span>) <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> cupy.dtype(cupy.float32).itemsize))</span></code></pre>
</div>
<p>As you may have noticed, we had to retrieve the size in bytes of the
data type <code>cupy.float32</code>, and this is done with
<code>cupy.dtype(cupy.float32).itemsize</code>.</p>
<p>After these changes, the body of the kernel needs to be modified to
use the right indices:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>__global__ <span class="dt">void</span> vector_add<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>  <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>  <span class="dt">int</span> offset <span class="op">=</span> threadIdx<span class="op">.</span>x <span class="op">*</span> <span class="dv">3</span><span class="op">;</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>  <span class="kw">extern</span> __shared__ <span class="dt">float</span> temp<span class="op">[];</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span> item <span class="op">&lt;</span> size <span class="op">)</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>  <span class="op">{</span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>      temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">=</span> A<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>      temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">1</span><span class="op">]</span> <span class="op">=</span> B<span class="op">[</span>item<span class="op">];</span></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a>      temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">2</span><span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">0</span><span class="op">]</span> <span class="op">+</span> temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">1</span><span class="op">];</span></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>      C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> temp<span class="op">[</span>offset <span class="op">+</span> <span class="dv">2</span><span class="op">];</span></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>And for completeness, we present the full Python code.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># vector size</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># GPU memory allocation</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>gpu_args <span class="op">=</span> (a_gpu, b_gpu, c_gpu, size)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="co"># CPU memory allocation</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a>a_cpu <span class="op">=</span> cupy.asnumpy(a_gpu)</span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a>b_cpu <span class="op">=</span> cupy.asnumpy(b_gpu)</span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a>c_cpu <span class="op">=</span> np.zeros(size, dtype<span class="op">=</span>np.float32)</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a>vector_add_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a><span class="vs">  int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a><span class="vs">  int offset = threadIdx.x * 3;</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="vs">  extern __shared__ float temp[];</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a><span class="vs">  if ( item &lt; size )</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a><span class="vs">  {</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a><span class="vs">      temp[offset + 0] = A[item];</span></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a><span class="vs">      temp[offset + 1] = B[item];</span></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a><span class="vs">      temp[offset + 2] = temp[offset + 0] + temp[offset + 1];</span></span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a><span class="vs">      C[item] = temp[offset + 2];</span></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a><span class="vs">  }</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a><span class="co"># compile and execute code</span></span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(vector_add_cuda_code, <span class="st">"vector_add"</span>)</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>vector_add_gpu(grid_size, block_size, gpu_args, shared_mem<span class="op">=</span>(threads_per_block <span class="op">*</span> <span class="dv">3</span> <span class="op">*</span> cupy.dtype(cupy.float32).itemsize))</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a><span class="co"># execute Python code and compare results</span></span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>vector_add(a_cpu, b_cpu, c_cpu, size)</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>np.allclose(c_cpu, c_gpu)</span></code></pre>
</div>
<p>The code is now correct, although it is still not very useful. We are
definitely using shared memory, and we are using it the correct way, but
there is no performance gain we achieved by doing so. Actually, we are
making our code slower, not faster, because shared memory is slower than
registers.</p>
<p>Let us, therefore, work on an example where using shared memory is
actually useful. We start again with some Python code.</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">def</span> histogram(input_array, output_array):</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> input_array:</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>        output_array[item] <span class="op">=</span> output_array[item] <span class="op">+</span> <span class="dv">1</span></span></code></pre>
</div>
<p>The <code>histogram</code> function, as the name suggests, computes
the histogram of an array of integers, i.e. counts how many instances of
each integer are in <code>input_array</code>, and writes the count in
<code>output_array</code>. We can now generate some data and run the
code.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>input_array <span class="op">=</span> np.random.randint(<span class="dv">256</span>, size<span class="op">=</span><span class="dv">2048</span>, dtype<span class="op">=</span>np.int32)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>output_array <span class="op">=</span> np.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>np.int32)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>histogram(input_array, output_array)</span></code></pre>
</div>
<p>Everything as expected. We can now write the equivalent code in
CUDA.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>    output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">=</span> output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<div id="challenge-error-in-the-histogram" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-error-in-the-histogram" class="callout-inner">
<h3 class="callout-title">Challenge: error in the histogram<a class="anchor" aria-label="anchor" href="#challenge-error-in-the-histogram"></a>
</h3>
<div class="callout-content">
<p>If you look at the CUDA <code>histogram</code> code, there is a
logical error that prevents it to produce the correct results. Can you
find it?</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>    output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">=</span> output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]</span> <span class="op">+</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>The GPU is a highly parallel device, executing multiple threads at
the same time. In the previous code different threads could be updating
the same output item at the same time, producing wrong results.</p>
</div>
</div>
</div>
</div>
<p>To solve this problem, we need to use a function from the CUDA
library named <code>atomicAdd</code>. This function ensures that the
increment of <code>output_array</code> happens in an atomic way, so that
there are no conflicts in case multiple threads want to update the same
item at the same time.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>And the full Python code snippet.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="kw">def</span> histogram(input_array, output_array):</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> input_array:</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>        output_array[item] <span class="op">=</span> output_array[item] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="co"># input size</span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">25</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a><span class="co"># allocate memory on CPU and GPU</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>input_gpu <span class="op">=</span> cupy.random.randint(<span class="dv">256</span>, size<span class="op">=</span>size, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>input_cpu <span class="op">=</span> cupy.asnumpy(input_gpu)</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>output_gpu <span class="op">=</span> cupy.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>output_cpu <span class="op">=</span> cupy.asnumpy(output_gpu)</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>histogram_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a><span class="vs">__global__ void histogram(const int * input, int * output)</span></span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(output[input[item]]), 1);</span></span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb13-28"><a href="#cb13-28" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb13-29"><a href="#cb13-29" tabindex="-1"></a></span>
<span id="cb13-30"><a href="#cb13-30" tabindex="-1"></a><span class="co"># compile and setup CUDA code</span></span>
<span id="cb13-31"><a href="#cb13-31" tabindex="-1"></a>histogram_gpu <span class="op">=</span> cupy.RawKernel(histogram_cuda_code, <span class="st">"histogram"</span>)</span>
<span id="cb13-32"><a href="#cb13-32" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb13-33"><a href="#cb13-33" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-34"><a href="#cb13-34" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb13-35"><a href="#cb13-35" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" tabindex="-1"></a><span class="co"># check correctness</span></span>
<span id="cb13-37"><a href="#cb13-37" tabindex="-1"></a>histogram(input_cpu, output_cpu)</span>
<span id="cb13-38"><a href="#cb13-38" tabindex="-1"></a>histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))</span>
<span id="cb13-39"><a href="#cb13-39" tabindex="-1"></a><span class="cf">if</span> np.allclose(output_cpu, output_gpu):</span>
<span id="cb13-40"><a href="#cb13-40" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb13-41"><a href="#cb13-41" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb13-42"><a href="#cb13-42" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span>
<span id="cb13-43"><a href="#cb13-43" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" tabindex="-1"></a><span class="co"># measure performance</span></span>
<span id="cb13-45"><a href="#cb13-45" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> histogram(input_cpu, output_cpu)</span>
<span id="cb13-46"><a href="#cb13-46" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(histogram_gpu, (grid_size, block_size, (input_gpu, output_gpu)), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-47"><a href="#cb13-47" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb13-48"><a href="#cb13-48" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>The CUDA code is now correct, and computes the same result as the
Python code; it is also faster than the Python code, as you can see from
measuring the execution time. However, we are accumulating the results
directly in global memory, and the more conflicts we have in global
memory, the lower the performance of our <code>histogram</code> will be.
Moreover, the access pattern to the output array is very irregular,
being dependent on the content of the input array. GPUs are designed for
very regular computations, and so if we can make the histogram more
regular we can hope in a further improvement in performance.</p>
<p>As you may expect, we can improve the memory access pattern by using
shared memory.</p>
<div id="challenge-use-shared-memory-to-speed-up-the-histogram" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-use-shared-memory-to-speed-up-the-histogram" class="callout-inner">
<h3 class="callout-title">Challenge: use shared memory to speed up the
histogram<a class="anchor" aria-label="anchor" href="#challenge-use-shared-memory-to-speed-up-the-histogram"></a>
</h3>
<div class="callout-content">
<p>Implement a new version of the CUDA <code>histogram</code> function
that uses shared memory to reduce conflicts in global memory. Modify the
following code and follow the suggestions in the comments.</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>    <span class="co">// Declare temporary histogram in shared memory</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>    <span class="dt">int</span> temp_histogram<span class="op">[</span><span class="dv">256</span><span class="op">];</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>    <span class="co">// Update the temporary histogram in shared memory</span></span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>    atomicAdd<span class="op">();</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>    <span class="co">// Update the global histogram in global memory, using the temporary histogram</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>    atomicAdd<span class="op">();</span></span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>Hint: for this exercise, you can safely assume that the size of
<code>output</code> is the same as the number of threads in a block.</p>
<p>Hint: <code>atomicAdd</code> can be used on both global and shared
memory.</p>
</div>
</div>
</div>
<div id="accordionSolution4" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution4" aria-expanded="false" aria-controls="collapseSolution4">
  <h4 class="accordion-header" id="headingSolution4">Show me the solution</h4>
</button>
<div id="collapseSolution4" class="accordion-collapse collapse" data-bs-parent="#accordionSolution4" aria-labelledby="headingSolution4">
<div class="accordion-body">
<p>The following code shows one of the possible solutions.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a>    __shared__ <span class="dt">int</span> temp_histogram<span class="op">[</span><span class="dv">256</span><span class="op">];</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>temp_histogram<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]),</span> temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]);</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>The idea behind this solution is to reduce the expensive conflicts in
global memory by having a temporary histogram in shared memory. After a
block has finished processing its fraction of the input array, and the
local histogram is populated, threads collaborate to update the global
histogram. Not only this solution potentially reduces the conflicts in
global memory, it also produces a better access pattern because threads
read adjacent items of the <code>input</code> array, and write to
adjacent elements of the <code>output</code> array during the second
call to <code>atomicAdd</code>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="thread-synchronization">Thread Synchronization<a class="anchor" aria-label="anchor" href="#thread-synchronization"></a>
</h1>
<p>There is still one potentially big issue in the
<code>histogram</code> code we just wrote, and the issue is that shared
memory is not coherent without explicit synchronization. The problem
lies in the following two lines of code:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>atomicAdd<span class="op">(&amp;(</span>temp_histogram<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]),</span> temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]);</span></span></code></pre>
</div>
<p>In the first line each thread updates one arbitrary position in
shared memory, depending on the value of the input, while in the second
line each thread reads the element in shared memory corresponding to its
thread ID. However, the changes to shared memory are not automatically
available to all other threads, and therefore the final result may not
be correct.</p>
<p>To solve this issue, we need to explicitly synchronize all threads in
a block, so that memory operations are also finalized and visible to
all. To synchronize threads in a block, we use the
<code>__syncthreads()</code> CUDA function. Moreover, shared memory is
not initialized, and the programmer needs to take care of that too. So
we need to first initialize <code>temp_histogram</code>, wait that all
threads are done doing this, perform the computation in shared memory,
wait again that all threads are done, and only then update the global
array.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>__global__ <span class="dt">void</span> histogram<span class="op">(</span><span class="dt">const</span> <span class="dt">int</span> <span class="op">*</span> input<span class="op">,</span> <span class="dt">int</span> <span class="op">*</span> output<span class="op">)</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>    __shared__ <span class="dt">int</span> temp_histogram<span class="op">[</span><span class="dv">256</span><span class="op">];</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a> </span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>    <span class="co">// Initialize shared memory and synchronize</span></span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>    temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]</span> <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a>    __syncthreads<span class="op">();</span></span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>    <span class="co">// Compute shared memory histogram and synchronize</span></span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>temp_histogram<span class="op">[</span>input<span class="op">[</span>item<span class="op">]]),</span> <span class="dv">1</span><span class="op">);</span></span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>    __syncthreads<span class="op">();</span></span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>    <span class="co">// Update global histogram</span></span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a>    atomicAdd<span class="op">(&amp;(</span>output<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]),</span> temp_histogram<span class="op">[</span>threadIdx<span class="op">.</span>x<span class="op">]);</span></span>
<span id="cb17-17"><a href="#cb17-17" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>And the full Python code snippet.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="kw">def</span> histogram(input_array, output_array):</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> input_array:</span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>        output_array[item] <span class="op">=</span> output_array[item] <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="co"># input size</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">25</span></span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a><span class="co"># allocate memory on CPU and GPU</span></span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a>input_gpu <span class="op">=</span> cupy.random.randint(<span class="dv">256</span>, size<span class="op">=</span>size, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a>input_cpu <span class="op">=</span> cupy.asnumpy(input_gpu)</span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>output_gpu <span class="op">=</span> cupy.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>output_cpu <span class="op">=</span> cupy.asnumpy(output_gpu)</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>histogram_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a><span class="vs">__global__ void histogram(const int * input, int * output)</span></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a><span class="vs">    __shared__ int temp_histogram[256];</span></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a><span class="vs"> </span></span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a><span class="vs">    // Initialize shared memory and synchronize</span></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a><span class="vs">    temp_histogram[threadIdx.x] = 0;</span></span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a><span class="vs">    __syncthreads();</span></span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a><span class="vs">    // Compute shared memory histogram and synchronize</span></span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(temp_histogram[input[item]]), 1);</span></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a><span class="vs">    __syncthreads();</span></span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a></span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a><span class="vs">    // Update global histogram</span></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(output[threadIdx.x]), temp_histogram[threadIdx.x]);</span></span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb18-39"><a href="#cb18-39" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" tabindex="-1"></a><span class="co"># compile and setup CUDA code</span></span>
<span id="cb18-41"><a href="#cb18-41" tabindex="-1"></a>histogram_gpu <span class="op">=</span> cupy.RawKernel(histogram_cuda_code, <span class="st">"histogram"</span>)</span>
<span id="cb18-42"><a href="#cb18-42" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb18-43"><a href="#cb18-43" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-44"><a href="#cb18-44" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-45"><a href="#cb18-45" tabindex="-1"></a></span>
<span id="cb18-46"><a href="#cb18-46" tabindex="-1"></a><span class="co"># check correctness</span></span>
<span id="cb18-47"><a href="#cb18-47" tabindex="-1"></a>histogram(input_cpu, output_cpu)</span>
<span id="cb18-48"><a href="#cb18-48" tabindex="-1"></a>histogram_gpu(grid_size, block_size, (input_gpu, output_gpu))</span>
<span id="cb18-49"><a href="#cb18-49" tabindex="-1"></a><span class="cf">if</span> np.allclose(output_cpu, output_gpu):</span>
<span id="cb18-50"><a href="#cb18-50" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb18-51"><a href="#cb18-51" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb18-52"><a href="#cb18-52" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span>
<span id="cb18-53"><a href="#cb18-53" tabindex="-1"></a></span>
<span id="cb18-54"><a href="#cb18-54" tabindex="-1"></a><span class="co"># measure performance</span></span>
<span id="cb18-55"><a href="#cb18-55" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> histogram(input_cpu, output_cpu)</span>
<span id="cb18-56"><a href="#cb18-56" tabindex="-1"></a>execution_gpu <span class="op">=</span> benchmark(histogram_gpu, (grid_size, block_size, (input_gpu, output_gpu)), n_repeat<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-57"><a href="#cb18-57" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(execution_gpu.gpu_times)</span>
<span id="cb18-58"><a href="#cb18-58" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span></code></pre>
</div>
<p>While both versions of the GPU histogram are correct, the one using
shared memory is faster; but how fast? On a NVIDIA Tesla T4 accessed via
Google Colab, the shared memory version is ten times faster than the
version doing atomic operations on global memory.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“Shared memory is faster than global memory and local memory”</li>
<li>“Shared memory can be used as a user-controlled cache to speedup
code”</li>
<li>“Size of shared memory arrays must be known at compile time if
allocated inside a thread”</li>
<li>“It is possible to declare <code>extern</code> shared memory arrays
and pass the size during kernel invocation”</li>
<li>“Use <code>__shared__</code> to allocate memory in the shared memory
space”</li>
<li>“Use <code>__syncthreads()</code> to wait for shared memory
operations to be visible to all threads in a block”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-constant_memory"><p>Content from <a href="constant_memory.html">Constant Memory</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/constant_memory.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“Is there a way to have a read-only cache in CUDA?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Understanding when and how to use constant memory”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="constant-memory">Constant Memory<a class="anchor" aria-label="anchor" href="#constant-memory"></a>
</h1>
<p>Constant memory is a read-only cache which content can be broadcasted
to multiple threads in a block. A variable allocated in constant memory
needs to be declared in CUDA by using the special
<code>__constant__</code> identifier, and it must be a global variable,
i.e. it must be declared in the scope that contains the kernel, not
inside the kernel itself. If all of this sounds complex do not worry, we
are going to see how this works with an example.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">C<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode c" tabindex="0"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="kw">extern</span> <span class="st">"C"</span> <span class="op">{</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="pp">#define BLOCKS </span><span class="dv">2</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>__constant__ <span class="dt">float</span> factors<span class="op">[</span>BLOCKS<span class="op">];</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>__global__ <span class="dt">void</span> sum_and_multiply<span class="op">(</span><span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> A<span class="op">,</span> <span class="dt">const</span> <span class="dt">float</span> <span class="op">*</span> B<span class="op">,</span> <span class="dt">float</span> <span class="op">*</span> C<span class="op">,</span> <span class="dt">const</span> <span class="dt">int</span> size<span class="op">)</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>    <span class="dt">int</span> item <span class="op">=</span> <span class="op">(</span>blockIdx<span class="op">.</span>x <span class="op">*</span> blockDim<span class="op">.</span>x<span class="op">)</span> <span class="op">+</span> threadIdx<span class="op">.</span>x<span class="op">;</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>    C<span class="op">[</span>item<span class="op">]</span> <span class="op">=</span> <span class="op">(</span>A<span class="op">[</span>item<span class="op">]</span> <span class="op">+</span> B<span class="op">[</span>item<span class="op">])</span> <span class="op">*</span> factors<span class="op">[</span>blockIdx<span class="op">.</span>x<span class="op">];</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="op">}</span></span></code></pre>
</div>
<p>In the previous code snippet we implemented a kernel that, given two
vectors <code>A</code> and <code>B</code>, stores their element-wise sum
in a third vector, <code>C</code>, scaled by a certain factor; this
factor is the same for all threads in the same thread block. Because
these factors are shared, i.e. all threads in the same thread block use
the same factor for scaling their sums, it is a good idea to use
constant memory for the <code>factors</code> array. In fact you can see
that the definition of <code>factors</code> is preceded by the
<code>__constant__</code> keyword, and said definition is in the global
scope. It is important to note that the size of the constant array needs
to be known at compile time, therefore the use of the
<code>define</code> preprocessor statement. On the kernel side there is
no need to do more, the <code>factors</code> vector can be normally
accessed inside the code as any other vector, and because it is a global
variable it does not need to be passed to the kernel as a function
argument.</p>
<p>The initialization of constant memory happens on the host side, and
we show how this is done in the next code snippet.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># compile the code</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>module <span class="op">=</span> cupy.RawModule(code<span class="op">=</span>cuda_code)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co"># allocate and copy constant memory</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>factors_ptr <span class="op">=</span> module.get_global(<span class="st">"factors"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>factors_gpu <span class="op">=</span> cupy.ndarray(<span class="dv">2</span>, cupy.float32, factors_ptr)</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>factors_gpu[...] <span class="op">=</span> cupy.random.random(<span class="dv">2</span>, dtype<span class="op">=</span>cupy.float32)</span></code></pre>
</div>
<p>From the previous code it is clear that dealing with constant memory
is a slightly more verbose affair than usual. First, we need to compile
the code, that in this case is contained in a Python string named
<code>cuda_code</code>. This is necessary because constant memory is
defined in the CUDA code, so we need CUDA to allocate the necessary
memory, and then provide us with a pointer to this memory. By calling
the method <code>get_global</code> we ask the CUDA subsystem to provide
us with the location of a global object, in this case the array
<code>factors</code>. We can then create our own CuPy array and point
that to the object returned by <code>get_global</code>, so that we can
use it in Python as we would normally do. Note that we use the constant
<code>2</code> for the size of the array, the same number we are using
in the CUDA code; it is important that we use the same number or we may
end up accessing memory that is outside the bound of the array. Lastly,
we initialize the array with some random floating point numbers.</p>
<div id="challenge-print-the-content-of-constant-memory" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-print-the-content-of-constant-memory" class="callout-inner">
<h3 class="callout-title">Challenge: print the content of constant
memory<a class="anchor" aria-label="anchor" href="#challenge-print-the-content-of-constant-memory"></a>
</h3>
<div class="callout-content">
<p>What should be the output of the following line of code?</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="bu">print</span>(factors_gpu)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>In our case the output of this line of code is two floating point
numbers, e.g. <code>[0.11390183 0.2585096 ]</code>. However, we are not
really accessing the content of the GPU’s constant memory from the host,
we are simply accessing the host-side copy of the data maintained by
CuPy.</p>
</div>
</div>
</div>
</div>
<p>We can now combine all the code together and execute it.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># size of the vectors</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">2048</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co"># allocating and populating the vectors</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co"># prepare arguments</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>args <span class="op">=</span> (a_gpu, b_gpu, c_gpu, size)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a><span class="vs">extern "C" {</span></span>
<span id="cb4-14"><a href="#cb4-14" tabindex="-1"></a><span class="vs">#define BLOCKS 2</span></span>
<span id="cb4-15"><a href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" tabindex="-1"></a><span class="vs">__constant__ float factors[BLOCKS];</span></span>
<span id="cb4-17"><a href="#cb4-17" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" tabindex="-1"></a><span class="vs">__global__ void sum_and_multiply(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb4-19"><a href="#cb4-19" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb4-20"><a href="#cb4-20" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb4-21"><a href="#cb4-21" tabindex="-1"></a><span class="vs">    C[item] = (A[item] + B[item]) * factors[blockIdx.x];</span></span>
<span id="cb4-22"><a href="#cb4-22" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb4-23"><a href="#cb4-23" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb4-24"><a href="#cb4-24" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb4-25"><a href="#cb4-25" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" tabindex="-1"></a><span class="co"># compile and access the code</span></span>
<span id="cb4-27"><a href="#cb4-27" tabindex="-1"></a>module <span class="op">=</span> cupy.RawModule(code<span class="op">=</span>cuda_code)</span>
<span id="cb4-28"><a href="#cb4-28" tabindex="-1"></a>sum_and_multiply <span class="op">=</span> module.get_function(<span class="st">"sum_and_multiply"</span>)</span>
<span id="cb4-29"><a href="#cb4-29" tabindex="-1"></a><span class="co"># allocate and copy constant memory</span></span>
<span id="cb4-30"><a href="#cb4-30" tabindex="-1"></a>factors_ptr <span class="op">=</span> module.get_global(<span class="st">"factors"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" tabindex="-1"></a>factors_gpu <span class="op">=</span> cupy.ndarray(<span class="dv">2</span>, cupy.float32, factors_ptr)</span>
<span id="cb4-32"><a href="#cb4-32" tabindex="-1"></a>factors_gpu[...] <span class="op">=</span> cupy.random.random(<span class="dv">2</span>, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb4-33"><a href="#cb4-33" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" tabindex="-1"></a>sum_and_multiply((<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), (size <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>), args)</span></code></pre>
</div>
<p>As you can see the code is not very general, it uses constants and
works only with two blocks, but it is a working example of how to use
constant memory.</p>
<div id="challenge-generalize-the-previous-code" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-generalize-the-previous-code" class="callout-inner">
<h3 class="callout-title">Challenge: generalize the previous code<a class="anchor" aria-label="anchor" href="#challenge-generalize-the-previous-code"></a>
</h3>
<div class="callout-content">
<p>Have a look again at the code using constant memory, and make it
general enough to be able to run on input of arbitrary size. Experiment
with some different input sizes.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<p>One of the possible solutions is the following one.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># size of the vectors</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">10</span><span class="op">**</span><span class="dv">6</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># allocating and populating the vectors</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a><span class="co"># prepare arguments</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>args <span class="op">=</span> (a_gpu, b_gpu, c_gpu, size)</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a><span class="co"># CUDA code</span></span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a><span class="vs">extern "C" {</span></span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a><span class="vs">__constant__ float factors[BLOCKS];</span></span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a><span class="vs">__global__ void sum_and_multiply(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb5-19"><a href="#cb5-19" tabindex="-1"></a><span class="vs">    if ( item &lt; size )</span></span>
<span id="cb5-20"><a href="#cb5-20" tabindex="-1"></a><span class="vs">    {</span></span>
<span id="cb5-21"><a href="#cb5-21" tabindex="-1"></a><span class="vs">        C[item] = (A[item] + B[item]) * factors[blockIdx.x];</span></span>
<span id="cb5-22"><a href="#cb5-22" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb5-23"><a href="#cb5-23" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb5-24"><a href="#cb5-24" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb5-25"><a href="#cb5-25" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb5-26"><a href="#cb5-26" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" tabindex="-1"></a><span class="co"># compute the number of blocks and replace "BLOCKS" in the CUDA code</span></span>
<span id="cb5-28"><a href="#cb5-28" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb5-29"><a href="#cb5-29" tabindex="-1"></a>num_blocks <span class="op">=</span> <span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block))</span>
<span id="cb5-30"><a href="#cb5-30" tabindex="-1"></a>cuda_code <span class="op">=</span> cuda_code.replace(<span class="st">"BLOCKS"</span>, <span class="ss">f"</span><span class="sc">{</span>num_blocks<span class="sc">}</span><span class="ss">"</span>) </span>
<span id="cb5-31"><a href="#cb5-31" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" tabindex="-1"></a><span class="co"># compile and access the code</span></span>
<span id="cb5-33"><a href="#cb5-33" tabindex="-1"></a>module <span class="op">=</span> cupy.RawModule(code<span class="op">=</span>cuda_code)</span>
<span id="cb5-34"><a href="#cb5-34" tabindex="-1"></a>sum_and_multiply <span class="op">=</span> module.get_function(<span class="st">"sum_and_multiply"</span>)</span>
<span id="cb5-35"><a href="#cb5-35" tabindex="-1"></a><span class="co"># allocate and copy constant memory</span></span>
<span id="cb5-36"><a href="#cb5-36" tabindex="-1"></a>factors_ptr <span class="op">=</span> module.get_global(<span class="st">"factors"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" tabindex="-1"></a>factors_gpu <span class="op">=</span> cupy.ndarray(num_blocks, cupy.float32, factors_ptr)</span>
<span id="cb5-38"><a href="#cb5-38" tabindex="-1"></a>factors_gpu[...] <span class="op">=</span> cupy.random.random(num_blocks, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb5-39"><a href="#cb5-39" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" tabindex="-1"></a>sum_and_multiply((num_blocks, <span class="dv">1</span>, <span class="dv">1</span>), (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>), args)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“Globally scoped arrays, which size is known at compile time, can be
stored in constant memory using the <code>__constant__</code>
identifier”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section><section id="aio-streams"><p>Content from <a href="streams.html">Concurrent access to the GPU</a></p>
<hr>
<p>Last updated on 2024-03-12 |
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/episodes/streams.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>“Is it possible to concurrently execute more than one kernel on a
single GPU?”</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>“Understand how to use CUDA streams and events”</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level1">
<h1 id="concurrently-execute-two-kernels-on-the-same-gpu">Concurrently execute two kernels on the same GPU<a class="anchor" aria-label="anchor" href="#concurrently-execute-two-kernels-on-the-same-gpu"></a>
</h1>
<p>So far we only focused on completing one operation at the time on the
GPU, writing and executing a single CUDA kernel each time. However the
GPU has enough resources to perform more than one task at the same
time.</p>
<p>Let us assume that, for our program, we need to compute both a list
of prime numbers, and a histogram, two kernels that we developed in this
same lesson. We could write both kernels in CUDA, and then execute them
on the GPU, as shown in the following code.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> cupyx.profiler <span class="im">import</span> benchmark</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>upper_bound <span class="op">=</span> <span class="dv">100_000</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>histogram_size <span class="op">=</span> <span class="dv">2</span><span class="op">**</span><span class="dv">25</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># GPU code</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>check_prime_gpu_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a><span class="vs">__global__ void all_primes_to(int size, int * const all_prime_numbers)</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="vs">    int number = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="vs">    int result = 1;</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="vs">    if ( number &lt; size )</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="vs">    {</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="vs">        for ( int factor = 2; factor &lt;= number / 2; factor++ )</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="vs">        {</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="vs">            if ( number </span><span class="sc">% f</span><span class="vs">actor == 0 )</span></span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="vs">            {</span></span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a><span class="vs">                result = 0;</span></span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a><span class="vs">                break;</span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a><span class="vs">            }</span></span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a><span class="vs">        }</span></span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="vs">        all_prime_numbers[number] = result;</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="vs">    }</span></span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a>histogram_cuda_code <span class="op">=</span> <span class="vs">r'''</span></span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a><span class="vs">__global__ void histogram(const int * input, int * output)</span></span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb1-36"><a href="#cb1-36" tabindex="-1"></a><span class="vs">    int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb1-37"><a href="#cb1-37" tabindex="-1"></a><span class="vs">    __shared__ int temp_histogram[256];</span></span>
<span id="cb1-38"><a href="#cb1-38" tabindex="-1"></a><span class="vs"> </span></span>
<span id="cb1-39"><a href="#cb1-39" tabindex="-1"></a><span class="vs">    // Initialize shared memory and synchronize</span></span>
<span id="cb1-40"><a href="#cb1-40" tabindex="-1"></a><span class="vs">    temp_histogram[threadIdx.x] = 0;</span></span>
<span id="cb1-41"><a href="#cb1-41" tabindex="-1"></a><span class="vs">    __syncthreads();</span></span>
<span id="cb1-42"><a href="#cb1-42" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" tabindex="-1"></a><span class="vs">    // Compute shared memory histogram and synchronize</span></span>
<span id="cb1-44"><a href="#cb1-44" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(temp_histogram[input[item]]), 1);</span></span>
<span id="cb1-45"><a href="#cb1-45" tabindex="-1"></a><span class="vs">    __syncthreads();</span></span>
<span id="cb1-46"><a href="#cb1-46" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" tabindex="-1"></a><span class="vs">    // Update global histogram</span></span>
<span id="cb1-48"><a href="#cb1-48" tabindex="-1"></a><span class="vs">    atomicAdd(&amp;(output[threadIdx.x]), temp_histogram[threadIdx.x]);</span></span>
<span id="cb1-49"><a href="#cb1-49" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb1-50"><a href="#cb1-50" tabindex="-1"></a><span class="vs">'''</span></span>
<span id="cb1-51"><a href="#cb1-51" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" tabindex="-1"></a><span class="co"># Allocate memory</span></span>
<span id="cb1-53"><a href="#cb1-53" tabindex="-1"></a>all_primes_gpu <span class="op">=</span> cupy.zeros(upper_bound, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb1-54"><a href="#cb1-54" tabindex="-1"></a>input_gpu <span class="op">=</span> cupy.random.randint(<span class="dv">256</span>, size<span class="op">=</span>histogram_size, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb1-55"><a href="#cb1-55" tabindex="-1"></a>output_gpu <span class="op">=</span> cupy.zeros(<span class="dv">256</span>, dtype<span class="op">=</span>cupy.int32)</span>
<span id="cb1-56"><a href="#cb1-56" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" tabindex="-1"></a><span class="co"># Compile and setup the grid</span></span>
<span id="cb1-58"><a href="#cb1-58" tabindex="-1"></a>all_primes_to_gpu <span class="op">=</span> cupy.RawKernel(check_prime_gpu_code, <span class="st">"all_primes_to"</span>)</span>
<span id="cb1-59"><a href="#cb1-59" tabindex="-1"></a>grid_size_primes <span class="op">=</span> (<span class="bu">int</span>(math.ceil(upper_bound <span class="op">/</span> <span class="dv">1024</span>)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-60"><a href="#cb1-60" tabindex="-1"></a>block_size_primes <span class="op">=</span> (<span class="dv">1024</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-61"><a href="#cb1-61" tabindex="-1"></a>histogram_gpu <span class="op">=</span> cupy.RawKernel(histogram_cuda_code, <span class="st">"histogram"</span>)</span>
<span id="cb1-62"><a href="#cb1-62" tabindex="-1"></a>threads_per_block_hist <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb1-63"><a href="#cb1-63" tabindex="-1"></a>grid_size_hist <span class="op">=</span> (<span class="bu">int</span>(math.ceil(histogram_size <span class="op">/</span> threads_per_block_hist)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-64"><a href="#cb1-64" tabindex="-1"></a>block_size_hist <span class="op">=</span> (threads_per_block_hist, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-65"><a href="#cb1-65" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" tabindex="-1"></a><span class="co"># Execute the kernels</span></span>
<span id="cb1-67"><a href="#cb1-67" tabindex="-1"></a>all_primes_to_gpu(grid_size_primes, block_size_primes, (upper_bound, all_primes_gpu))</span>
<span id="cb1-68"><a href="#cb1-68" tabindex="-1"></a>histogram_gpu(grid_size_hist, block_size_hist, (input_gpu, output_gpu))</span>
<span id="cb1-69"><a href="#cb1-69" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" tabindex="-1"></a><span class="co"># Save results</span></span>
<span id="cb1-71"><a href="#cb1-71" tabindex="-1"></a>output_one <span class="op">=</span> all_primes_gpu</span>
<span id="cb1-72"><a href="#cb1-72" tabindex="-1"></a>output_two <span class="op">=</span> output_gpu</span></code></pre>
</div>
<p>In the previous code, after allocating memory and compiling, we
execute and measure the performance of one kernel, and when we are done
we do the same for the other kernel.</p>
<p>This is technically correct, but there is no reason why one kernel
should be executed before the other, because there is no dependency
between these two operations.</p>
<p>Therefore, while this is fine in our example, in a real application
we may want to run the two kernels concurrently on the GPU to reduce the
total execution time. To achieve this in CUDA we need to use CUDA
<em>streams</em>.</p>
<p>A stream is a sequence of GPU operations that is executed in order,
and so far we have been implicitly using the defaul stream. This is the
reason why all the operations we issued, such as data transfers and
kernel invocations, are performed in the order we specify them in the
Python code, and not in any other.</p>
<p>Have you wondered why after requesting data transfers to and from the
GPU, we do not stop to check if they are complete before performing
operations on such data? The reason is that within a stream all
operations are carried out in order, so the kernel calls in our code are
always performed after the data transfer from host to device is
complete, and so on.</p>
<p>If we want to create new CUDA streams, we can do it this way using
CuPy.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>stream_one <span class="op">=</span> cupy.cuda.Stream()</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>stream_two <span class="op">=</span> cupy.cuda.Stream()</span></code></pre>
</div>
<p>We can then execute the kernels in different streams by using the
Python <code>with</code> statement.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="cf">with</span> stream_one:</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>    all_primes_to_gpu(grid_size_primes, block_size_primes, (upper_bound, all_primes_gpu))</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="cf">with</span> stream_two:</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>    histogram_gpu(grid_size_hist, block_size_hist, (input_gpu, output_gpu))</span></code></pre>
</div>
<p>Using the <code>with</code> statement we implicitly execute the CUDA
operations in the code block using that stream. The result of doing this
is that the second kernel, i.e. <code>histogram_gpu</code>, does not
need to wait for <code>all_primes_to_gpu</code> to finish before being
executed.</p>
</div>
<div class="section level1">
<h1 id="stream-synchronization">Stream synchronization<a class="anchor" aria-label="anchor" href="#stream-synchronization"></a>
</h1>
<p>If we need to wait for all operations on a certain stream to finish,
we can call the <code>synchronize</code> method. Continuing with the
previous example, in the following Python snippet we wait for the
execution of <code>all_primes_to_gpu</code> on <code>stream_one</code>
to finish.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>stream_one.synchronize()</span></code></pre>
</div>
<p>This synchronization primitive is useful when we need to be sure that
all operations on a stream are finished, before continuing. It is,
however, a bit coarse grained. Imagine to have a stream with a whole
sequence of operations enqueued, and another stream with one data
dependency on one of these operations. If we use
<code>synchronize</code>, we wait until all operations of said stream
are completed before executing the other stream, thus negating the whole
reason of using streams in the first place.</p>
<p>A possible solution is to insert a CUDA <em>event</em> at a certain
position in the stream, and then wait specifically for that event.
Events are created in Python in the following way.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>interesting_event <span class="op">=</span> cupy.cuda.Event()</span></code></pre>
</div>
<p>And can then be added to a stream by using the <code>record</code>
method. In the following example we will create two streams: in the
first we will execute <code>histogram_gpu</code> twice, while in the
second one we will execute <code>all_primes_to_gpu</code> with the
condition that the kernel in the second stream is executed only after
the first histogram is computed.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>stream_one <span class="op">=</span> cupy.cuda.Stream()</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>stream_two <span class="op">=</span> cupy.cuda.Stream()</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>sync_point <span class="op">=</span> cupy.cuda.Event()</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="cf">with</span> stream_one:</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    all_primes_to_gpu(grid_size_primes, block_size_primes, (upper_bound, all_primes_gpu))</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    sync_point.record(stream<span class="op">=</span>stream_one)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>    all_primes_to_gpu(grid_size_primes, block_size_primes, (upper_bound, all_primes_gpu))</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="cf">with</span> stream_two:</span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>    stream_two.wait_event(sync_point)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>    histogram_gpu(grid_size_hist, block_size_hist, (input_gpu, output_gpu))</span></code></pre>
</div>
<p>With streams and events, it is possible to build up arbitrary
execution graphs for complex operations on the GPU.</p>
</div>
<div class="section level1">
<h1 id="measure-execution-time-using-streams-and-events">Measure execution time using streams and events<a class="anchor" aria-label="anchor" href="#measure-execution-time-using-streams-and-events"></a>
</h1>
<p>We can also use streams and events to measure the execution time of
kernels, without having to use the CuPy <code>benchmark</code> function.
In the following example, we go back to the <code>vector_add</code> code
and add code that uses events on the default stream to measure the
execution time.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># size of the vectors</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">100_000_000</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># allocating and populating the vectors</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>a_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a>b_gpu <span class="op">=</span> cupy.random.rand(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a>c_gpu <span class="op">=</span> cupy.zeros(size, dtype<span class="op">=</span>cupy.float32)</span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a>a_cpu <span class="op">=</span> cupy.asnumpy(a_gpu)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a>b_cpu <span class="op">=</span> cupy.asnumpy(b_gpu)</span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a>c_cpu <span class="op">=</span> np.zeros(size, dtype<span class="op">=</span>np.float32)</span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a><span class="co"># CPU code</span></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a><span class="kw">def</span> vector_add(A, B, C, size):</span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a>    <span class="cf">for</span> item <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, size):</span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a>        C[item] <span class="op">=</span> A[item] <span class="op">+</span> B[item]</span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a><span class="co"># CUDA vector_add</span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a>vector_add_gpu <span class="op">=</span> cupy.RawKernel(<span class="vs">r'''</span></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="vs">extern "C"</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a><span class="vs">__global__ void vector_add(const float * A, const float * B, float * C, const int size)</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a><span class="vs">{</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="vs">   int item = (blockIdx.x * blockDim.x) + threadIdx.x;</span></span>
<span id="cb7-27"><a href="#cb7-27" tabindex="-1"></a><span class="vs">   if ( item &lt; size )</span></span>
<span id="cb7-28"><a href="#cb7-28" tabindex="-1"></a><span class="vs">   {</span></span>
<span id="cb7-29"><a href="#cb7-29" tabindex="-1"></a><span class="vs">      C[item] = A[item] + B[item];</span></span>
<span id="cb7-30"><a href="#cb7-30" tabindex="-1"></a><span class="vs">   }</span></span>
<span id="cb7-31"><a href="#cb7-31" tabindex="-1"></a><span class="vs">}</span></span>
<span id="cb7-32"><a href="#cb7-32" tabindex="-1"></a><span class="vs">'''</span>, <span class="st">"vector_add"</span>)</span>
<span id="cb7-33"><a href="#cb7-33" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" tabindex="-1"></a><span class="co"># execute the code and measure time</span></span>
<span id="cb7-35"><a href="#cb7-35" tabindex="-1"></a>threads_per_block <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb7-36"><a href="#cb7-36" tabindex="-1"></a>grid_size <span class="op">=</span> (<span class="bu">int</span>(math.ceil(size <span class="op">/</span> threads_per_block)), <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-37"><a href="#cb7-37" tabindex="-1"></a>block_size <span class="op">=</span> (threads_per_block, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-38"><a href="#cb7-38" tabindex="-1"></a><span class="op">%</span>timeit <span class="op">-</span>n <span class="dv">1</span> <span class="op">-</span>r <span class="dv">1</span> vector_add(a_cpu, b_cpu, c_cpu, size)</span>
<span id="cb7-39"><a href="#cb7-39" tabindex="-1"></a>gpu_times <span class="op">=</span> []</span>
<span id="cb7-40"><a href="#cb7-40" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">10</span>):</span>
<span id="cb7-41"><a href="#cb7-41" tabindex="-1"></a>    start_gpu <span class="op">=</span> cupy.cuda.Event()</span>
<span id="cb7-42"><a href="#cb7-42" tabindex="-1"></a>    end_gpu <span class="op">=</span> cupy.cuda.Event()</span>
<span id="cb7-43"><a href="#cb7-43" tabindex="-1"></a>    start_gpu.record()</span>
<span id="cb7-44"><a href="#cb7-44" tabindex="-1"></a>    vector_add_gpu(grid_size, block_size, (a_gpu, b_gpu, c_gpu, size))</span>
<span id="cb7-45"><a href="#cb7-45" tabindex="-1"></a>    end_gpu.record()</span>
<span id="cb7-46"><a href="#cb7-46" tabindex="-1"></a>    end_gpu.synchronize()</span>
<span id="cb7-47"><a href="#cb7-47" tabindex="-1"></a>    gpu_times.append(cupy.cuda.get_elapsed_time(start_gpu, end_gpu))</span>
<span id="cb7-48"><a href="#cb7-48" tabindex="-1"></a>gpu_avg_time <span class="op">=</span> np.average(gpu_times)</span>
<span id="cb7-49"><a href="#cb7-49" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>gpu_avg_time<span class="sc">:.6f}</span><span class="ss"> s"</span>)</span>
<span id="cb7-50"><a href="#cb7-50" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" tabindex="-1"></a><span class="co"># test</span></span>
<span id="cb7-52"><a href="#cb7-52" tabindex="-1"></a><span class="cf">if</span> np.allclose(c_cpu, c_gpu):</span>
<span id="cb7-53"><a href="#cb7-53" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Correct results!"</span>)</span>
<span id="cb7-54"><a href="#cb7-54" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb7-55"><a href="#cb7-55" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Wrong results!"</span>)</span></code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>“”</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/carpentries-incubator/lesson-gpu-programming/" class="external-link">Source</a></p>
				<p><a href="https://github.com/carpentries-incubator/lesson-gpu-programming/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:instructors@esciencecenter.nl">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.4" class="external-link">sandpaper (0.16.4)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.5" class="external-link">pegboard (0.7.5)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.2" class="external-link">varnish (1.0.2)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://carpentries-incubator.github.io/lesson-gpu-programming/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "high-performance computing, HPC, graphics processing units, GPU",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://carpentries-incubator.github.io/lesson-gpu-programming/aio.html",
  "identifier": "https://carpentries-incubator.github.io/lesson-gpu-programming/aio.html",
  "dateCreated": "2020-09-25",
  "dateModified": "2024-05-07",
  "datePublished": "2024-05-07"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

